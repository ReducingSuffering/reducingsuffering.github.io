---
layout: article
title: "Цель не оправдывает средства (среди людей)"
authors:
  - Элиезер Юдковский
original_date: 2008.10.15
original: https://www.readthesequences.com/Ends-Dont-Justify-Means-Among-Humans
translated_by: К. Кирдан
edited_by: Александр Бидерман
license:
  - CC BY-NC-SA 3.0
  - https://creativecommons.org/licenses/by-nc-sa/3.0/deed.ru
preview: assets/images/previews/detective-7021757.jpg
---
> «Если не цель оправдывает средства, то что?»<br>
> — приписывается разным источникам
>
> «Я рассматриваю себя как сущность, запущенную на враждебном оборудовании.»<br>
> — Джастин Корвин

Люди, возможно, эволюционно унаследовали структуру политической революции, при которой всё начинается с убежденности в собственном моральном превосходстве над нынешней коррумпированной системой власти, но заканчивается тем, что [власть развращает самих революционеров](https://www.lesswrong.com/posts/v8rghtzWCziYuMdJ5/why-does-power-corrupt) — не из-за того, что они это планировали, а из-за отголоска предков, которые поступали также и в итоге размножались.

Это соответствует шаблону:

> В некоторых случаях люди эволюционировали так, чтобы думать, что они делают X по просоциальной причине Y, но когда они действительно делают X, срабатывают другие адаптации, ведущие к достижению выгодных лично для них последствий Z.

Отсюда я перейду к моему главному вопросу, который _в значительной степени_ выходит за рамки классической байесовской теории принятия решений:

> Что, если я работаю на испорченном оборудовании?

В подобном случае вы можете делать даже такие, казалось бы, парадоксальные утверждения (полную чепуху с точки зрения классической теории принятия решений!), как:

> Цель не оправдывает средства.

Ведь если вы запущены на испорченном оборудовании, то внутреннее представление о том, что захват вами власти _выглядит_ праведным и альтруистичным поступком, может не являться достаточным свидетельством в пользу того, что захват власти и вправду является тем, что принесет племени наибольшую пользу.

Благодаря силе наивного реализма испорченное оборудование, на котором вы работаете, и испорченные представления, которые оно вычисляет, будут казаться тканью самой реальности — просто тем, каковы вещи есть на самом деле.

И вот, мы получаем странно выглядящее правило: «ради блага своего племени не жульничайте ради захвата власти, _даже если племя от этого выиграет_».

В самом деле, возможно, мудрой будет именно такая формулировка. Если сказать просто «когда _кажется_, что это принесет племени чистую выгоду», то обязательно появятся люди, которые скажут «но так не просто _кажется_ — если я стану самым главным, племя _действительно_ от этого выиграет».

Понятие ненадежного оборудования выглядит чем-то совершенно выходящим за рамки классической теории принятия решений. (Я пока не могу сказать, как это влияет на рефлексивную теорию принятия решений, но, похоже, это проблема подходящего для неё уровня.)

Но на человеческом уровне кажется, что проблему легко решить. Как только вы обнаруживаете искажение, вы создаёте правила, в которых искаженное поведение описывается и объявляется незаконным. Правило, которое гласит «ради блага племени не жульничайте ради захвата власти, даже ради блага племени». Или «ради блага племени, не убивайте даже ради блага племени».

И вот приходит философ и представляет свой «мысленный эксперимент»: создаёт сценарий, в котором, _по условиям_, _единственный_ возможный способ спасти пять невинных жизней — это убить одного невинного человека, и это убийство _действительно_ спасло бы пять жизней. «Вагонетка едет по пути, на котором она собьёт пятерых невинных людей, и вы не можете предупредить их, чтобы они ушли с её дороги, но вы можете столкнуть одного невинного человека под вагонетку, и это её остановит. Других вариантов нет. Что будете делать?»

Столкнувшись с этим мысленным экспериментом, альтруистичный человек, принявший определенные деонтологические запреты (которые выглядят вполне оправданными, учитывая исторические статистические данные о последствиях определенного рода рассуждений на ненадёжном оборудовании), может испытать душевный дискомфорт.

Итак, вот ответ на сценарий этого философа, которого я ещё не слышал от его жертв:

«Вы утверждаете, что _единственный возможный_ способ спасти пять невинных жизней — это убить одного невинного человека, и что это убийство _действительно_ спасёт пять жизней, и что эти факты достоверно мне _известны_. Но поскольку я работаю на испорченном оборудовании, я не могу находиться в том _эпистемическом состоянии_, которое вы хотите, чтобы я представил. Поэтому я отвечу, что в том обществе, которое состоит из искусственных интеллектов, считающихся личностями и лишенных какой-либо врожденной склонности быть развращенными властью, для ИИ было бы правильно убить одного невинного ради спасения пятерых. И что, более того, все его соплеменники с этим бы согласились. Однако я отказываюсь распространять этот ответ на себя, потому что эпистемическое состояние, которое вы просите меня представить, может существовать только среди существ, не являющихся людьми.»

Сейчас это кажется мне уловкой. Я думаю, что вселенная [достаточно жестока](https://www.lesswrong.com/lw/uk/beyond_the_reach_of_god/), чтобы справедливо вынудить нас рассматривать такие задачи. Люди, предлагающие подобные мысленные эксперименты, вполне могут заслуживать такого ответа, какой дан выше. Но в любой человеческой правовой системе на самом деле есть какой-то ответ на вопрос «сколько невинных людей мы можем посадить в тюрьму ради привлечения к ответственности виновных?», даже если это число нигде не записано.

Как человек, я стараюсь соблюдать деонтологические запреты, которые люди установили ради того, чтобы жить в мире друг с другом. Но я не думаю, что наши деонтологические запреты _буквально, по своей сути, в обход консеквенциализма, являются терминально правильными_. Я поддерживаю принцип «цель не оправдывает средства» как принцип для людей, работающих на испорченном оборудовании, но я бы не одобрил его как принцип для общества, состоящего из искусственных интеллектов, производящих хорошо выверенные оценки. (Если вы рассматриваете случай, где в обществе людей есть один ИИ, это потребует других соображений, — например, о том, учатся ли люди на вашем примере.)

Поэтому я не сказал бы, что хорошо спроектированный Дружественный ИИ (ДИИ) обязательно должен отказаться сталкивать человека с моста, чтобы остановить вагонетку. Очевидно, я ожидал бы, что любой нормальный сверхразум предложит третью альтернативу, которая ещё лучше. Но предположим, что есть действительно лишь два варианта, и ДИИ считает, что разумнее столкнуть одного человека с моста — даже с учетом побочных эффектов, связанных с тем, что кто-то это увидит, будет об этом рассказывать и так далее. В этом случае я не посчитал бы опасным сигналом, если бы _ИИ_ заявил, что правильное решение — пожертвовать одним ради спасения пяти. Да, сам я не сталкиваю людей на пути, и не ворую деньги у банков ради финансирования своих альтруистических проектов. _Я_ родился человеком. Но Дружественный ИИ не может оказаться развращённым властью — это всё равно как если бы он [начал истекать красной кровью](https://lesswrong.ru/w/%D0%9B%D1%8E%D0%B4%D0%B8_%D0%B2_%D1%81%D0%BC%D0%B5%D1%88%D0%BD%D1%8B%D1%85_%D0%BD%D0%B0%D1%80%D1%8F%D0%B4%D0%B0%D1%85). Склонность к развращению властью — это специфическая биологическая адаптация, которая поддерживается особыми когнитивными контурами, заложенными в нас нашими генами по очевидной эволюционной причине. Эта склонность не появится спонтанно в коде Дружественного ИИ. Во всяком случае, не раньше, чем его транзисторы начнут кровоточить.

Я пошел бы ещё дальше и сказал, что если бы речь шла об умах со встроенным искажением, заставляющим их _переоценивать_ вред окружающим от поступков, приносящих лично им пользу, то им потребовалось бы правило «цель не запрещает средства» — о том, что вы должны делать то, что приносит вам пользу, даже если (кажется, что) это вредит племени. Согласно предположению, если бы в их обществе не было такого правила, они отказались бы дышать из страха использовать чужой кислород, и все бы вымерли. Случайное излишество, при котором кто-то из них извлекает личную выгоду за счёт сообщества, могло бы казаться для них столь же осторожно добродетельным (и действительно _было бы_ столь же осторожно добродетельным), как когда один из нас, людей, из осторожности упускает возможность украсть буханку хлеба, которая на самом деле принесла бы ему больше пользы, чем убытка для торговца (даже учитывая побочные эффекты).

«Цель не оправдывает средства» — это просто консеквенциалистское рассуждение на один метауровень выше. Если на _предметном_ уровне человек начнёт думать, что цель оправдывает средства, это будет иметь ужасные последствия, учитывая наш ненадёжный мозг. Поэтому человек не должен так думать. Но в конечном итоге всё это — по-прежнему консеквенциализм. Это просто _рефлексивный_ консеквенциализм для существ, которые понимают, что их ежесекундные решения принимаются ненадёжным оборудованием.
