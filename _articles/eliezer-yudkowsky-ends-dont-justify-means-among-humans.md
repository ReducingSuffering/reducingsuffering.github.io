---
layout: article
title: "Цель не оправдывает средства (среди людей)"
authors:
  - Элиезер Юдковский
original_date: 2008.10.15
original: https://www.readthesequences.com/Ends-Dont-Justify-Means-Among-Humans
translated_by: К. Кирдан
license:
  - CC BY-NC-SA 3.0
  - https://creativecommons.org/licenses/by-nc-sa/3.0/deed.ru
preview: assets/images/previews/
---
> «Если не цель оправдывает средства, то что?»<br>
> — приписывается разным источникам
>
> «Думаю, я работаю на враждебном оборудовании.»<br>
> — Джастин Корвин

Люди, возможно, эволюционно унаследовали структуру политической революции, при которой всё начинается с убежденности в собственном моральном превосходстве над нынешней коррумпированной системой власти, но заканчивается тем, что [власть развращает самих революционеров](https://www.lesswrong.com/posts/v8rghtzWCziYuMdJ5/why-does-power-corrupt) — не из-за того, что они это планировали, а из-за отголоска предков, которые поступали также и в итоге размножались.

Это соответствует шаблону:

> В некоторых случаях люди эволюционировали так, чтобы думать, что они делают X по просоциальной причине Y, но когда они действительно делают X, срабатывают другие адаптации, ведущие к достижению выгодных лично для них последствий Z.

Отсюда я перейду к моему главному вопросу, который _в значительной степени_ выходит за рамки классической байесовской теории принятия решений:

> Что, если я работаю на испорченном оборудовании?

В подобном случае вы можете делать даже такие, казалось бы, парадоксальные утверждения (полную чепуху с точки зрения классической теории принятия решений!), как:

> Цель не оправдывает средства.

Но если вы работаете на испорченном оборудовании, то внутреннее представление о том, что захват вами власти _кажется_ праведным и альтруистичным поступком, может не служить достаточным доказательством утверждения о том, что захват власти и вправду является тем, что принесет племени наибольшую пользу.

Благодаря силе наивного реализма испорченное оборудование, на котором вы работаете, и испорченные представления, которые оно вычисляет, будут казаться тканью самой реальности — просто тем, каковы вещи есть на самом деле.

И вот, мы получаем странно выглядящее правило: «ради блага своего племени не жульничайте ради захвата власти, _даже если это принесло бы племени чистую выгоду_».

В самом деле, возможно, мудро будет формулировать это именно так. Если вы скажете просто «когда _кажется_, что это принесет племени чистую выгоду», то встретите людей, которые скажут «но так не просто _кажется_ — это _действительно_ принесло бы племени чистую выгоду, если бы я стал главным».

Понятие ненадежного оборудования выглядит чем-то совершенно выходящим за рамки классической теории принятия решений. (Я пока не могу сказать, как это влияет на рефлексивную теорию принятия решений, но, похоже, это проблема подходящего для неё уровня.)

Но на человеческом уровне патч кажется простым. Как только вы обнаруживаете искажение, вы создаёте правила, в которых искаженное поведение описывается и объявляется незаконным. Правило, которое гласит «ради блага племени не жульничайте ради захвата власти, даже ради блага племени». Или «ради блага племени, не убивайте даже ради блага племени».

И вот приходит философ и представляет свой «мысленный эксперимент»: создаёт сценарий, в котором, _по условиям_, _единственный_ возможный способ спасти пять невинных жизней — это убить одного невинного человека, и это убийство _действительно_ спасло бы пять жизней. «Вагонетка едет по пути, на котором она собьёт пятерых невинных людей, и вы не можете предупредить их, чтобы они ушли с её дороги, но вы можете столкнуть одного невинного человека под вагонетку, и это её остановит. Других вариантов нет. Что будете делать?»

Столкнувшись с этим мысленным экспериментом, альтруистичный человек, принявший определенные деонтологические запреты (которые выглядят вполне оправданными, учитывая исторические статистические данные о последствиях определенного рода рассуждений на ненадёжном оборудовании), может испытать некоторую тревогу.

Итак, вот ответ на сценарий этого философа, которого я ещё не слышал от его жертв:

«Вы утверждаете, что _единственный возможный_ способ спасти пять невинных жизней — это убить одного невинного человека, и что это убийство _действительно_ спасёт пять жизней, и что эти факты достоверно мне _известны_. Но поскольку я работаю на испорченном оборудовании, я не могу находиться в том _эпистемическом состоянии_, которое вы хотите, чтобы я представил. Поэтому я отвечу, что в том обществе, которое состоит из искусственных интеллектов, считающихся личностями и лишенных какой-либо врожденной склонности быть развращенными властью, для ИИ было бы правильно убить одного невинного ради спасения пятерых. И что, более того, все его соплеменники с этим бы согласились. Однако я отказываюсь распространять этот ответ на себя, потому что эпистемическое состояние, которое вы просите меня представить, может существовать только среди существ, не являющихся людьми.»

Сейчас это кажется мне уловкой. Я думаю, что вселенная [достаточно жестока](https://www.lesswrong.com/lw/uk/beyond_the_reach_of_god/), чтобы мы могли быть справедливо вынуждены рассматривать ситуации такого рода. Люди, предлагающие подобные мысленные эксперименты, вполне могут заслуживать такого ответа, какой дан выше. Но в любой человеческой правовой системе на самом деле содержится какой-то ответ на вопрос «сколько невинных людей мы можем посадить в тюрьму ради привлечения к ответственности виновных?», даже если это число нигде не записано.

Как человек, я стараюсь соблюдать деонтологические запреты, которые люди установили ради того, чтобы жить в мире друг с другом. Но я не думаю, что наши деонтологические запреты _буквально, по своей сути, в обход консеквенциализма, являются терминально правильными_. Я поддерживаю принцип «цель не оправдывает средства» как принцип для людей, работающих на испорченном оборудовании, но я бы не одобрил его как принцип для общества, состоящего из искусственных интеллектов, производящих хорошо выверенные оценки. (Если вы рассматриваете случай, где в обществе людей есть один ИИ, это потребует других соображений, — например, о том, учатся ли люди на вашем примере.)

Поэтому я не сказал бы, что хорошо спроектированный Дружественный ИИ (ДИИ) обязательно должен отказаться сталкивать того человека с уступа, чтобы остановить вагонетку. Очевидно, я ожидал бы, что любой порядочный сверхразум предложит третью альтернативу, которая ещё лучше. Но если варианта лишь два, и ДИИ считает, что разумнее столкнуть одного человека с уступа — даже с учетом побочных эффектов на людей, которые увидят это и распрострят историю об этом и т. д. — тогда я не скажу, что было бы дурным сигналом, если бы _ИИ_ говорил, что правильное решение — пожертвовать одним ради спасения пяти. Опять же, сам я не сталкиваю людей на пути, как и не ворую у банков ради финансирования своих альтруистических проектов. _Я_ родился человеком. Но вот для Дружественного ИИ оказаться развращённым властью — это всё равно как если бы он [начал истекать красной кровью](https://lesswrong.ru/w/%D0%9B%D1%8E%D0%B4%D0%B8_%D0%B2_%D1%81%D0%BC%D0%B5%D1%88%D0%BD%D1%8B%D1%85_%D0%BD%D0%B0%D1%80%D1%8F%D0%B4%D0%B0%D1%85). Склонность к развращению властью — это специфическая биологическая адаптация, которая поддерживается особыми когнитивными контурами, заложенными в нас нашими генами по очевидной эволюционной причине. Эта склонность не появится спонтанно в коде Дружественного ИИ раньше, чем его транзисторы начнут кровоточить.

Я пошел бы ещё дальше и сказал, что если бы речь шла об умах со встроенным искажением, заставляющим их _переоценивать_ вред окружающим от поступков, приносящих лично им пользу, то им потребовалось бы правило «цель не запрещает средства» — о том, что вы должны делать то, что приносит вам пользу, даже если (кажется, что) это вредит племени. Согласно предположению, если бы в их обществе не было такого правила, они отказались бы дышать из страха использовать чужой кислород, и все бы вымерли. Случайное излишество, при котором кто-то из них извлекает личную выгоду за счёт сообщества, могло бы казаться для них столь же осторожно добродетельным (и действительно _было бы_ столь же осторожно добродетельным), как когда один из нас, людей, из осторожности упускает возможность украсть буханку хлеба, которая на самом деле принесла бы ему больше пользы, чем убытка для торговца (даже учитывая побочные эффекты).

«Цель не оправдывает средства» — это просто консеквенциалистское рассуждение на один метауровень выше. Если на _предметном_ уровне человек начнёт думать, что цель оправдывает средства, это будет иметь ужасные последствия, учитывая наш ненадёжный мозг. Поэтому человек не должен так думать. Но в конечном итоге всё это — по-прежнему консеквенциализм. Это просто _рефлексивный_ консеквенциализм для существ, которые понимают, что их ежесекундные решения принимаются ненадёжным оборудованием.
