---
layout: article
title: Нет универсально убедительных аргументов
authors:
  - Элиезер Юдковский
original_date: 2008.06.26
original: https://www.readthesequences.com/No-Universally-Compelling-Arguments
translated_by: К. Кирдан
license:
  - CC BY-NC-SA 3.0
  - https://creativecommons.org/licenses/by-nc-sa/3.0/deed.ru
preview: assets/images/previews/brain-7170513-.jpg
preview_here: true
---
Что такого _пугающего_ в идее, что не все возможные умы могут согласиться с нами, даже в принципе?

Для некоторых — ничего, их это нисколько не беспокоит. И некоторых из _этих_ людей это не беспокоит по той _причине_, что у них нет четкого представления о стандартах и ​​истинах, выходящих за рамки личных прихотей. Если они говорят, что небо голубое или что убийство неправильно, это просто их личное мнение; и то, что у кого-то другого может быть другое мнение, их не удивляет.

Другие люди не могут принять существования разногласий, которые сохраняются даже _в принципе_. И некоторых из _этих_ людей это беспокоит по той _причине_, что им кажется, что если вы допускаете, что некоторых людей невозможно _даже в принципе_ убедить в том, что небо голубое, то вы признаете, что «небо голубое» — это просто _произвольное_ личное мнение.

Я [предложил](https://www.readthesequences.com/TheDesignSpaceOfMindsInGeneral) вам не поддаваться искушению обобщать что-либо на все пространство возможных устройств разума. Если мы ограничимся разумами, которые можно определить в рамках триллиона бит или меньше, то каждое _универсальное_ обобщение «Для всех разумов _m: X (m)_» имеет шанс два к триллиону быть ложным, в то время как каждое _экзистенциальное_ обобщение «Существует разум _m: X (m)_» имеет шанс два к триллиону оказаться правдой.

Казалось бы, это доказывает, что для каждого аргумента А, каким бы убедительным он нам ни казался, существует по крайней мере один возможный ум, на который этот аргумент не подействует.

<a id="citationa"></a>
И неожиданность и/или кошмарность этой перспективы (для некоторых), думаю, во многом связана с интуицией о [духе-в-машине](https://lesswrong.ru/w/%D0%9F%D1%80%D0%B8%D0%B7%D1%80%D0%B0%D0%BA%D0%B8_%D0%B2_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%B5)<sup>[a](#footnotea)</sup> — духе с неким нередуцируемым ядром, которое можно убедить любым _действительно веским_ аргументом.

Ранее я [говорил](https://lesswrong.ru/w/%D0%9F%D1%80%D0%B8%D0%B7%D1%80%D0%B0%D0%BA%D0%B8_%D0%B2_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%B5) об интуиции, следуя которой люди [ассоциируют](https://www.greaterwrong.com/lw/rj/surface_analogies_and_deep_causes/) _программирование компьютера_ с _инструктированием человека-прислуги_, как будто компьютер может восстать против своего кода — или, возможно, просмотреть код, решить, что он неразумен, и вернуть его обратно.

Если бы в машине был дух, и этот дух содержал бы в себе нередуцируемое ядро ​​разумности, за пределами которого любой простой код был бы лишь предложением, тогда могли бы существовать универсальные аргументы. Даже если бы духу изначально были переданы коды-предложения, противоречащие Универсальному Аргументу, то когда мы, наконец, подвергли бы духа воздействию Универсального Аргумента (или дух мог бы обнаружить Универсальный Аргумент самостоятельно — это тоже популярная идея), дух просто отменил бы собственный ошибочный исходный код.

Но как однажды сказал студент-программист: «У меня такое ощущение, что компьютер просто игнорирует все мои комментарии». Код не передан ИИ; код — это и _есть_ ИИ.

Если переключиться на взгляд с точки зрения физики, то идея Универсального Аргумента покажется заметно нефизической. Если есть физическая система, которая в момент T после воздействия аргумента E выполняет X, то должна быть и другая физическая система, которая в момент T после воздействия среды E выполняет Y. Любая мысль должна быть _где-то_ реализована, в физической системе; любое убеждение, любой вывод, любое решение, любая двигательная активность. Для каждой следующей законам каузальной системы, которая совершает какие-то движения во множестве точек, вы должны быть в состоянии описать другую каузальную систему, которая следуя законам совершала бы обратные движения в тех же точках.

Допустим, есть разум с транзистором, который выдает +3 вольта в момент времени T, что означает, что он только что согласился с каким-то убедительным аргументом. Тогда мы можем построить очень похожую физическую когнитивную систему с крошечным люком под транзистором, в котором сидит маленький серый человечек, который вылезает из него в момент T и устанавливает выходной сигнал этого транзистора на -3 вольта, что означает несогласие. В этом нет ничего акаузального; маленький серый человечек находится здесь, потому что мы его сюда встроили. Идея аргумента, который убеждает _любой_ разум, похоже, подразумевает наличие маленькой синей женщины, которая _никогда_ не была встроена в систему, но вылезает буквально из _ниоткуда_ и душит маленького серого человечка, потому что этот транзистор только что _должен_ был выдать +3 вольта. Понимаете, это вот _настолько убедительный аргумент_.

Но принуждение — это не свойство аргументов; это [свойство умов](https://lesswrong.ru/w/%D0%9E%D1%88%D0%B8%D0%B1%D0%BA%D0%B0_%D0%BF%D1%80%D0%BE%D0%B5%D1%86%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F_%D1%83%D0%BC%D0%B0), обрабатывающих аргументы.

Так что причина, по которой я выступаю против идеи духа, заключается не _просто_ в том, чтобы подчеркнуть, что (1) Дружественный ИИ должен быть явным образом запрограммирован и (2) законы физики не запрещают возможность создания Дружественного ИИ. (Хотя, конечно, у меня есть определенный интерес в установлении этих фактов.)

Я также хочу установить понятие разума как _каузальной_, _подчиняющейся законам_, _физической системы_, в центре которой _нет_ нередуцируемого духа, который просматривает ее нейроны/код и решает, хороши ли они.

<a id="citationb"></a>
(Есть такая идея как _намеренное_ программирование Дружественного ИИ так, чтобы он проверял собственный исходный код и, возможно, возвращал бы его программистам. Но проверяющий себя разум не является нередуцируемым, — это всего лишь разум, который вы создали. ДИИ перенормирует себя, _но он и был таким разработан_ сам по себе; нет ничего акаузального, проникающего в него извне. Бутстрэп, а не скайхук<sup>[b](#footnoteb)</sup>.)

Все это возвращает к [беспокойству](https://lesswrong.ru/w/%D0%90%D0%BF%D1%80%D0%B8%D0%BE%D1%80%D0%B8) о «произвольности» [априорных убеждений](https://www.greaterwrong.com/lw/hk/priors_as_mathematical_objects/) в байесовском подходе. Если вы покажете мне одного байесианца, который вытягивает из бочки 4 красных шара и 1 белый шар и приписывает получению красного шара в следующий раз вероятность 5/7 (следуя правилу последовательности Лапласа), то я могу показать вам другой разум, который тоже подчиняется правилу Байеса, но делает вывод о том, что вероятность вытянуть красный в следующий раз равна 2/7, — следуя другому априорному убеждению о бочке, хоть, возможно, и менее «обоснованному».

Многие философы убеждены, что, раз в принципе можно построить априорное убеждение, которое будет обновляться до любого наперед заданного вывода после обработки потока свидетельств, то байесианские рассуждения «произвольны» и весь план байесианства ошибочен, поскольку опирается на «необоснованные» предположения, и что в действительности это «ненаучно», — потому что вы не можете заставить любого возможного редактора журнала из пространства возможных разумов согласиться с вами.

И это убеждение, как я уже отвечал, основано на идее, что, раскрыв все аргументы и их обоснования, вы можете получить [идеального студента-философа совершенной пустоты](https://lesswrong.ru/w/%D0%90%D0%BF%D1%80%D0%B8%D0%BE%D1%80%D0%B8), которого убедит линия рассуждений, которая начинается с абсолютного отсутствия каких-либо предположений.

Но кто этот идеальный философ совершенной пустоты? Да ведь это просто нередуцируемое ядро духа!

И именно поэтому, продолжал я, результатом попытки удалить из разума все предпосылки и раскрыться до полного отсутствия каких-либо априорных убеждений будет не идеальный философ совершенной пустоты, а камень. Что останется в сознании после удаления исходного кода? Не дух, просматривающий исходный код, а просто . . . никакого духа.

Итак — и я еще подниму эту тему позднее — где бы у вас ни располагались ваши представления о _валидности_, или _ценности_, или _рациональности_, или _обоснованности_, или даже _объективности_, они не могут опираться на аргумент, который _универсально убедителен для всех физически возможных разумов_.

Также вы не способны определить валидность последовательностью обоснований, которая убеждает совершенную пустоту, начиная с ничего.

<a id="citation1"></a>
О, ну могут быть последовательности аргументов, которые убедили бы любого _человека_ без неврологических повреждений — например, аргумент, который я использую, чтобы убедить людей [выпустить ИИ из коробки](http://yudkowsky.net/singularity/aibox/)<sup>[1](#footnote1)</sup> — но это вряд ли тоже самое с философской точки зрения.

Первая крупная неудача тех, кто пытается рассматривать идею Дружественного ИИ, — это «Один Великий Моральный Принцип, Который Нам Достаточно Запрограммировать», т.е. [ложная функция полезности](https://lesswrong.ru/w/%D0%A4%D0%B0%D0%BB%D1%8C%D1%88%D0%B8%D0%B2%D1%8B%D0%B5_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8_%D0%BF%D0%BE%D0%BB%D0%B5%D0%B7%D0%BD%D0%BE%D1%81%D1%82%D0%B8) — и об этом я уже говорил.

Но провал еще хуже — это «Один Великий Моральный Принцип, Который Нам Даже _Не Нужно_ Программировать, Потому Что Любой ИИ Неизбежно Должен Вывести Его». Эта идея вызывает пугающе нездоровое очарование у тех, кто спонтанно переизобретает ее; они мечтают о приказах, которым не может не подчиниться ни один достаточно развитый разум. Сами боги провозгласят правоту их философии! (Например, Джон К. Райт, Марк Геддес.)

Есть также менее тяжелая версия провала, при которой человек не _провозглашает_ Единой Истинной Морали. Вместо этого он надеется на то, что ИИ будет создан _совершенно свободным_, не ограниченным несовершенными, желающими создать себе рабов людьми, — чтобы ИИ мог достичь добродетели по собственному желанию — добродетели, о которой, возможно, и не мечтал говорящий, который признает себя слишком несовершенным, чтобы обучать ИИ. (Например, Джон К. Кларк, Ричард Холлерит?, [Элиезер<sub>1996</sub>](https://lesswrong.ru/w/%D0%9C%D0%BE%D1%8F_%D0%B4%D0%B8%D0%BA%D0%B0%D1%8F_%D0%B8_%D0%B1%D0%B5%D0%B7%D0%B1%D0%B0%D1%88%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D1%8E%D0%BD%D0%BE%D1%81%D1%82%D1%8C).) Это менее испорченный мотив, чем мечта об абсолютном командовании. Но хотя _эта_ мечта порождена добродетелью, а не пороком, она все же основана на ошибочном понимании [свободы](https://www.greaterwrong.com/lw/rc/the_ultimate_source/), и на самом деле не _сработает в реальной жизни_. К этому, конечно, еще вернемся.

<a id="citationc"></a>
Джон К. Райт, который ранее писал очень хорошую трансгуманистическую трилогию (первая книга: «_Золотой век_»), вставил огромного «Автора-Флибустьера»<sup>[c](#footnotec)</sup> в середину своей решающей третьей книги, описывая на десятках страниц свою «Универсальную Мораль, Которая Должна Убедить Любой ИИ». Не знаю, произошло ли что-нибудь после этого, потому что я перестал читать. А потом Райт обратился в христианство — да, серьезно. Так что вы _действительно_ не хотели бы попасть в эту ловушку!

---

<a id="footnote1"></a>1\. Просто шутка. <a href="#citation1">↩︎</a>

<a id="footnotea"></a>a\. "Дух в машине" (или "призрак в машине") — это введенный философом [Гилбертом Райлом](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B9%D0%BB,_%D0%93%D0%B8%D0%BB%D0%B1%D0%B5%D1%80%D1%82) ("Понятие сознания", 1949) термин, который указывает на представление о разуме как о независимой от тела сущности, управляющей им — прим. пер. <a href="#citationa">↩︎</a>

<a id="footnoteb"></a>b\. Под "бутстрэпом" (англ. bootstrap) автор скорее всего имеет в виду самоулучшение без посторонней помощи (см. в [Cambridge Dictionary](https://dictionary.cambridge.org/dictionary/english/bootstrap)), а под "скайхуком" (англ. skyhook) метафорический крюк, используемый для подъема чего-либо на тросе, свисающем с неба без какой-либо поддержки (см. [Skyhook (cable)](https://en.wikipedia.org/wiki/Skyhook_(cable)) — такой воображаемый крюк используется в шутках) — прим. пер. <a href="#citationb">↩︎</a>

<a id="footnotec"></a>c\. "Автор-флибустьер" (или "авторский флибустьер") — это, как объясняют разные энциклопедии тропов (напр. [TVTropes](https://tvtropes.org/pmwiki/pmwiki.php/Main/AuthorFilibuster)), прием, при котором ход сюжета останавливается автором ради проповеди личного послания читателям, зачастую носящего весьма политический или этический характер — прим. пер. <a href="#citationc">↩︎</a>
