---
layout: article
title: Мысли о паузе в разработках ИИ
authors:
  - Магнус Виндинг
translated_by: К. Кирдан
original: https://magnusvinding.com/2024/06/06/thoughts-on-ai-pause/
original_date: "2024.06.06"
preview: https://magnusvinding.com/wp-content/uploads/2024/06/screen-shot-2024-06-06-at-11.11.52.png?w=1400
---
<center><img src="https://magnusvinding.com/wp-content/uploads/2024/06/screen-shot-2024-06-06-at-11.11.52.png?w=1400"/></center>

Вопрос о том, следует ли выступать за приостановку разработок ИИ, является предметом горячих споров. В этом посте я делюсь своими мыслями по поводу паузы в разработках ИИ и дискурса, который окружает эту идею.

## Содержание
1. [Мотивация к паузе в разработках ИИ](#Мотивация_к_паузе)
2. [Мои мысли насчет паузы в разработках ИИ, вкратце](#Мои_мысли_насчет_паузы)
3. [Мои мысли о дискурсе, окружающем паузу в разработках ИИ](#Мои_мысли_о_дискурсе_окружающем_паузу)
4. [Огромная моральная срочность: да, для обеих категорий рисков худших сценариев](#Огромная_моральная_срочность)

---
<a id="Мотивация_к_паузе"></a>
## Мотивация к паузе в разработках ИИ

В общем похоже, что основная мотивация призыва к [приостановке разработок ИИ](https://pauseai.info/) заключается в том, что работа над безопасностью ИИ ещё далека от того уровня, который необходим человечеству для сохранения контроля над будущим развитием ИИ. Поэтому требуется пауза, чтобы работа над безопасностью ИИ и другие связанные области, такие как управление ИИ — успели догнать темпы прогресса в развитии возможностей ИИ.

<a id="Мои_мысли_насчет_паузы"></a>
## Мои мысли насчет паузы в разработках ИИ, вкратце

Стоит ли добиваться паузы в разработках ИИ, очевидно, зависит от разных факторов. В частности, от альтернативных издержек: чем ещё вместо этого мы могли бы заняться? В конце концов, даже если кто-то считает, что пауза в разработках ИИ желательна, у него могут возникнуть сомнения насчет её достижимости по сравнению с другими целями. И даже если он считает, что пауза в разработках ИИ и желательна, и достижима, все еще могут быть другие цели и занятия, которые ещё более полезны (в математическом ожидании), такие как работа над безопасностью ИИ в наихудших сценариях (Gloor, [2016](https://longtermrisk.org/files/suffering-focused-ai-safety.pdf); Yudkowsky, [2017](https://arbital.greaterwrong.com/p/hyperexistential_separation/); Baumann, [2018](https://s-risks.org/an-introduction-to-worst-case-ai-safety/)) или увеличение приоритетности, отводимой людьми снижению рисков астрономических страданий (s-рисков) (Althaus & Gloor, [2016](https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/); Бауман [2017](tobias-baumann-s-risks-an-introduction.html); [2022](https://centerforreducingsuffering.org/wp-content/uploads/2022/10/Avoiding_The_Worst_final.pdf); DiGiovanni, [2021](https://forum.effectivealtruism.org/posts/RkPK8rWigSAybgGPe/a-longtermist-critique-of-the-expected-value-of-extinction-2)).

Кроме того, неясно, будет ли пауза в разработках ИИ вообще полезной. Это сложный вопрос, который я не буду подробно здесь рассматривать. (Для критики см. «[AI Pause Will Likely Backfire](https://forum.effectivealtruism.org/posts/JYEAL8g7ArqGoTaX6/ai-pause-will-likely-backfire)» Норы Белроуз.) Достаточно сказать, что, на мой взгляд, совершенно неясно, будет ли какая-нибудь реалистичная пауза в разработках ИИ в целом полезной — не только с точки зрения подхода, сфокусированного на уменьшении страданий, но и с точки зрения практически всех беспристрастных[^1] систем ценностей. Мне кажется, что большинство сторонников паузы в разработках ИИ чрезмерно уверены в ее полезности.

Но чтобы уточнить: я вовсе не выступаю против призывов к паузе в разработках ИИ. Это кажется мне чем-то, что можно по разумным причинам счесть полезным и стоящим усилий (в зависимости от ценностей и эмпирических оценок). Однако, по моим нынешним оценкам это просто вряд ли относится к лучшим способам уменьшения будущих страданий, главным образом потому, что я считаю перечисленные выше альтернативные варианты деятельности более перспективными, и потому что я подозреваю, что большинство реалистичных пауз в разработках ИИ вряд ли будут однозначно полезными в целом.

<a id="Мои_мысли_о_дискурсе_окружающем_паузу"></a>
## Мои мысли о дискурсе, окружающем паузу в разработках ИИ

Критическое замечание насчет многих обсуждений вокруг паузы в разработках ИИ состоит в том, что они часто сводятся к упрощённой дихотомии «гибель / не-гибель». То есть рисуется картина, согласно которой либо человечество теряет контроль над ИИ и вымирает, что плохо; либо сохраняет контроль над ИИ, что хорошо. И ваша оценка вероятности первого сценария — это ваша «p(doom)»[^2].

Разумеется, можно утверждать, что для стратегических и коммуникационных целей есть смысл упрощать и говорить в таких дихотомических терминах. Однако проблема, на мой взгляд, заключается в том, что такая картина неточна даже в первом приближении. С точки зрения альтруизма совсем не обязательно, что «потеря контроля над ИИ» = «плохо», а «сохранение контроля людьми» = «хорошо».

Например, если мы хотим снижения s-рисков (что важно с точки зрения практически любых беспристрастных систем ценностей), мы должны сравнивать риски «потери контроля над ИИ» с рисками «сохранения контроля людьми» — как бы мы ни определяли эти приблизительные категории. И, к сожалению, нельзя сказать, что «сохранение контроля людьми» связано с пренебрежимо малым или тривиальным риском наихудших исходов. В самом деле, неясно, связано ли «сохранение контроля людьми» в общем случае с более или менее благоприятными перспективами в плане снижения s-рисков, чем при «потере контроля над ИИ».

В целом вопрос о том, лучше ли «будущее под контролем людей» с точки зрения уменьшения будущих страданий, сложен, давно обсуждается и по нему нет явного консенсуса. Например, Брайан Томасик [дает](https://reducing-suffering.org/summary-beliefs-values-big-questions/) субъективную оценку вероятности того, что «управляемый людьми ИИ в среднем приведет к меньшему количеству страданий, чем неуправляемый», в 52%.

Такой близкий к 50/50 взгляд резко контрастирует с тем, что часто берут в качестве ключевой предпосылки в обсуждениях вокруг паузы в разработках ИИ, а именно, что ожидаемое будущее под контролем людей явно намного лучше.

(Некоторые причины, по которым можно быть пессимистичным насчет будущего под контролем людей, можно найти в литературе о моральных недостатках людей; см. напр., Cooper, [2018](https://www.google.dk/books/edition/Animals_and_Misanthropy/BnVUDwAAQBAJ?hl=en&gbpv=1&printsec=frontcover); Huemer, [2019](https://web.archive.org/web/20220320200847/https://fakenous.net/?p=272); Kidd, [2020](https://www.youtube.com/watch?v=rYr5LFGaauQ); Svoboda, [2022](https://philarchive.org/archive/SVOAPD). К другим причинам относятся базовые конкурентные цели и динамика, которые, скорее всего, будут в наличии в широком диапазоне сценариев будущего, включая управляемые людьми; см. например, Tomasik, [2013](https://reducing-suffering.org/the-future-of-darwinism/); Knutsson, [2022, разд. 3](https://www.simonknutsson.com/pessimism-value-future-welfare-acts-traits/#3_How_widespread_the_acts_and_traits_are_and_will_be). Также см. Vinding, [2022](https://magnusvinding.com/2022/09/09/beware-underestimating-bad-outcomes/).)

<a id="Огромная_моральная_срочность"></a>
## Огромная моральная срочность: да, для обеих категорий рисков худших сценариев

Есть ключевой момент, в котором я совершенно согласен со сторонниками паузы в разработках ИИ: [огромная моральная срочность](https://magnusvinding.com/2022/12/08/distrusting-salience-keeping-unseen-urgencies-in-mind/) избежания ужасных исходов будущего, контролируемого ИИ. Слишком мало людей осознают важность этого, и ещё меньше тех, кого это глубоко трогает.

В то же время я считаю, что есть такая же огромная срочность избежания ужасных исходов будущего, контролируемого людьми. И, к сожалению, нынешняя траектория развития человечества не слишком обнадёживает в контексте любого из этих широких классов рисков. (Чтобы прояснить: я имею в виду не то, что исход с s-рисками _наиболее вероятен_ в любом из этих двух классов сценариев будущего, а лишь то, что текущая траектория выглядит крайне неоптимальной и вызывающей опасения в контексте обоих классов сценариев.)

Мое заключение таково: есть примерно равная моральная срочность избежания каждой из этих категорий рисков худших исходов будущего. И, как я уже подсказывал ранее, мне кажется сомнительным, что призывы к паузе в разработках ИИ — это лучший способ снижения этих рисков в целом.

---

Примечания переводчика:

[^1]: т. е. удовлетворяющие [принципу равного рассмотрения интересов](https://www.utilitarianism.net/types-of-utilitarianism/#impartiality-and-the-equal-consideration-of-interests).

[^2]: «doom» — это «гибель» на англ.
