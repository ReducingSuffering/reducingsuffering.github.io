---
layout: article
title: "Краткий обзор на работу Эрика Дрекслера по рефреймингу безопасности ИИ"
authors:
  - Тобиас Бауман
translated_by: К. Кирдан
original: https://s-risks.org/summary-of-eric-drexlers-work-on-reframing-ai-safety/
original_date:
  - 2017 
  - 2020.05.21
preview: assets/images/previews/ai-generated-8001027-.jpg
preview_here: true
---
<div markdown="1">
В этом посте содержится краткое изложение основных положений работы «[Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)» Эрика Дрекслера. (Я написал его в 2017 году, так что оно не обязательно отражает самую актуальную версию его работы.)

Мне кажется, работа Дрекслера очень интересна, потому что у него довольно необычный взгляд на ИИ. Я считаю, что в его идеях есть определённая ценность, и мне нравится, что он ставит под сомнение ключевые предположения в этой сфере. Но я не уверен, что согласен со всеми деталями, и думаю, что нам нужно быть гораздо менее уверенными насчет ИИ, чем в его текстах зачастую (неявно) предполагается.

Ключевые идеи таковы:

- Он считает, что искусственный общий интеллект (AGI) не обязательно должен быть агентоподобным. Вместо этого мы можем создать «комплексные ИИ-услуги» (CAIS), которые будут сверхинтеллектуальными, но не будут действовать как непрозрачный агент.
- Он считает, что обычная концепция интеллекта ошибочна, и что ИИ радикально отличается от человеческого интеллекта.
- Он считает, что люди могут сохранить контроль над стратегическими решениями высокого уровня.<br><br>

Ниже я кратко опишу разделы, которые мне показались наиболее интересными.

---

[Модель автоматизации НИОКР для рекурсивного улучшения](https://drive.google.com/open?id=1zj6pJcZUuHFNPLGQGPzBvkreXAwbwyVwIFqA_yRMVIA):

- По мере развития ИИ мы можем ожидать, что ИИ-продукты будут автоматизировать выполняемые людьми задачи в процессе разработки ИИ, что позволит рекурсивно улучшать ИИ-технологии.
  - Но это не подразумевает рекурсивного самоулучшения агента.
  - Дрекслер считает, что можно получить комплексные, сверхинтеллектуальные ИИ-услуги без рисков, связанных с ИИ-агентами.
  - Исходя из этого, вы можете применять сверхинтеллектуальные услуги для решения проблем безопасности (связанных с ИИ-агентами).<br><br>

[Технологии внедрения AGI могут напрямую предоставить альтернативы AGI](https://drive.google.com/open?id=1mDXFSTaKaz2HFZXQQhCtwjBqNyhW8E01kRXMAMGWap8):

- Вместо непрозрачных, самоулучшающихся AGI-агентов можно внедрять открытые, комплексные ИИ-услуги.
- Самоулучшающиеся ИИ-системы общего назначения могут вносить вклад в разработку ИИ до того, как ИИ станет в общем плане сверхчеловеческим (включая моделирование мира, разработку планов его захвата и т.д.).
- Технологии, которые могут быть использованы для внедрения непрозрачных самоулучшающихся ИИ, могут также использоваться для других целей, например, в открытых системах.
  - Нет веских причин упаковывать и запечатывать процессы разработки ИИ в непрозрачный «ящик».
- Неожиданное и проблематичное поведение вполне возможно, но оно отличается от классических рисков AGI.
  - Он считает, что эта проблема более податлива, чем классическая «проблема контроля».<br><br>

[Системы с широкой компетенцией работают за счет координации более узких компетенций](https://drive.google.com/open?id=1UXHBccJ0BdFuV4MDTf5-qZnl_6TlKk9N6y_4wsLYzCE):

- И у людей, и у ИИ-систем более широкие способности строятся на основе более узких.
- Это игнорируется в абстракции ИИ как «чёрного ящика» (но в зависимости от задачи это может оказаться и полезным).<br><br>

[Конкурентное давление дает мало стимулов для передачи стратегических решений ИИ-системам](https://drive.google.com/open?id=14pqv6Lt61rNK6iPP39K301iZ81iB6jupsiNTs7MsFoI):

- В целом скорость и качество принятия решений ИИ будут способствовать передаче ИИ контроля над решениями.
- Стратегические решения высокого уровня — это решения с высокими ставками, и они менее срочны, чем многие другие решения; их могут оценивать люди.
  - Это значит, что люди могут пользоваться компетентностью ИИ, не уступая контроля, потому что ИИ-системы будут предлагать отличные варианты.
  - Дрекслер утверждает, что это не повредит конкуренции.
- Высшие из руководителей-людей, скорее всего, выберут сохранение своей власти. Передача ИИ контроля над стратегическими решениями высокого уровня увеличила бы риски при уменьшении выгод.
- _Я довольно скептично отношусь к этим утверждениям._<br><br>

[Модели рациональных агентов неявно придают интеллекту антропоморфный характер](https://drive.google.com/open?id=1LM6Yw3cISF4-z1m_0z1Ywjx1Wln8hgA0owf4AsWWlvk):

- Дрекслер утверждает, что обычная модель рационального агента психоморфна и антропоморфна.
- Рассматривать ИИ как разум интуитивно привлекательно, но это заблуждение.
- Даже модели рациональных агентов изначально являются идеализацией принятия решений людьми; они абстрагируют содержание человеческих умов, но сохраняют роль разума в управлении решениями.
  - Даже интеллект высокого уровня не обязан быть психоморфным.     
- Он утверждает, что даже в техническом анализе ИИ-систем часто содержатся биологические допущения.
- Разумо-подобные сверхинтеллекты могут быть аттрактором, но всё же важно моделировать всё пространство потенциальных ИИ-систем.    
- Дрекслер утверждает, что появляющиеся технологии ИИ радикально отличны от интеллектуальных систем, появившихся в результате эволюции:<br><br>
</div>

<div>
<table style="border: 1px solid black; border-collapse: collapse;">
<tr style="border: 1px solid black; border-collapse: collapse;"><th></th><th>Эволюционировавшие системы</th><th>Спроектированные системы</th></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Организация элементов</td><td>Отдельные организмы</td><td><a href="https://docs.google.com/document/d/1bQk9lCap2W5q7jV6u2u-AuPjKLqQoP5YadKIiWPhFXY/edit?usp=sharing">Системы из компонентов</a></td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Источник новых способностей</td><td>Постепенная эволюция</td><td>Проектирование систем</td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Источник конкретных экземпляров</td><td>Локальное размножение</td><td>Загрузка из репозитория</td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Основание для выполнения обучения</td><td>Индивидуальный опыт</td><td><a href="https://docs.google.com/document/d/1EXzkKvhbZneCPIxzLD6Cjh-KwM8w7_l-ouVliGyHARw/edit?usp=sharing">Агрегированные обучающие данные</a></td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Передача знаний</td><td>Обучение, имитация</td><td><a href="https://docs.google.com/document/d/1WtbDyBlIyq-tlXjgRqHnIDIE4dwAIvvtwA1j8aZPxa0/edit?usp=sharing">Копирование кода, данных</a></td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Требуемые компетенции</td><td>Общие навыки жизни</td><td>Выполнение специфического поручения</td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Метрика успешности</td><td>Репродуктивная приспособленность</td><td>Соответствие цели</td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Самомодификация</td><td>Необходимо</td><td>Опционально</td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Непрерывность существования</td><td>Необходимо</td><td>Опционально</td></tr>
<tr style="border: 1px solid black; border-collapse: collapse;"><td>Агентность, ориентированная на мир</td><td>Необходимо</td><td>Опционально</td></tr>
</table></div>

<div markdown="1">
[Стандартные определения «сверхинтеллекта» смешивают обучение с компетентностью](https://drive.google.com/open?id=1WtbDyBlIyq-tlXjgRqHnIDIE4dwAIvvtwA1j8aZPxa0):

- Интеллект — это способность к обучению, которую можно понимать как отдельную от компетентности.
- Эти два понятия следует различать как у людей, так и у ИИ-систем.
  - Ребёнок считается разумным из-за способности к обучению, эксперт считается умным из-за компетентности.
- Обучение и компетентность разделимы как в принципе, так и на практике.
- Паттерны обучения и компетентности радикально различаются у людей и типичных ИИ-систем.
  - Например, у людей обучение и компетентность объединены, но у ИИ-систем они разделимы.
  - Ошибочно полагать, что ИИ-системы со сверхчеловеческими навыками обучения обязательно будут обладать сверхчеловеческими способностями.<br><br>

[Предоставление комплексных ИИ-услуг не требует AGI и не влечет его за собой](https://drive.google.com/open?id=12_Abdc_FFXh35n8QBg51NxgIN18qK-AAUT2RgBtPQLY):

- Дрекслер считает, что практические стимулы для разработки AGI неожиданно слабы, потому что вместо AGI можно получить комплексные ИИ-услуги (и это будет гораздо безопаснее).
- Это значит, что разработка AGI не неизбежна (но всё же по умолчанию произойдет, если её не предотвратят активными действиями).<br><br>

[Системы с обучением с подкреплением ≠ агенты, стремящиеся к вознаграждению](https://drive.google.com/open?id=1DIV_RUL0Vum-WYYcbEHFyp9ayKsc2Aig769ANDF13GQ):

- В сегодняшней практике системы с обучением с подкреплением (RL-системы) обычно отличаются от агентов, которые они порождают.
  - RL-подкрепления поддерживают обучение, но не являются мотивацией / действием / источниками ценностей.
  - RL-системы и агенты, выполняющие поручения, не являются едиными «RL-агентами»; вместо этого обученные агенты — это продукты RL-систем.
- Агрегация ослабляет связь между агентами, действиями и опытом.
- Как и в других постах, важно рассматривать это с точки зрения «подхода, ориентированного на разработку».<br><br>

[Обширные знания о мире совместимы с сильным (и безопасным) фокусом на поручениях](https://drive.google.com/open?id=1kBhlFg7tSqwFTB73Ghq2NEo6so7pfJRN8-o0l6eLiSE):

- Системы машинного перевода показывают, что можно иметь систему с обширными знаниями о мире, но при этом ограниченную одной задачей.
- Нынешние нейросетевые системы машинного перевода развивают независимые от языка способы представления значений.
- Надёжный фокус на поручении может поддерживать безопасность.<br><br>

[Обеспечение отсутствия сговора между сверхинтеллектуальными оракулами](https://drive.google.com/open?id=1dD9khBi5HFpggwKb-XtKGz4fqLAp2-SV4wIMbdWUCsw):

- Он предлагает использовать сверхинтеллектуальные возможности решения задач для решения проблемы безопасности ИИ.
- Обычное возражение заключается в том, что такие подходы сами по себе небезопасны.
  - В частности, есть опасность, что различные системы вступят в сговор против людей.
- Но Дрекслер считает, что есть стратегии для предотвращения сговора об обмане.
- Пол Кристиано [тоже разделяет эту точку зрения](https://ai-alignment.com/on-heterogeneous-objectives-b38d0e003399).<br><br>

Прочие мысли о безопасности ИИ:

- Надзор людьми совместим с рекурсивным улучшением технологий ИИ и не вытесняется конкурентным давлением.
- ИИ-технологии сами могут помочь в обеспечении надзора людьми.
- Можно было бы [вылечить рак, не подвергаясь риску AGI](https://docs.google.com/document/d/1FQLeYMISpWnA6i_QtnYhJ9bdZCdhdGJN3pookZeneGQ/edit).
- Высокая сила оптимизации может повысить безопасность ИИ, ограничив структуру и поведение системы. 
- Идея запирания ИИ «в коробке» ошибочна, потому что возможности распределены.
- Безопасность ИИ следует рассматривать в контексте ожидаемых будущих знаний.
- Комплексные ИИ-услуги могут смягчить, но не решить проблему контроля над AGI.
</div>
