---
layout: article
title: "Почему мы можем ожидать, что у наших потомков не будет сострадания"
authors:
  - Джим Бюлер
original_date: 2023.07.10
original: https://forum.effectivealtruism.org/s/wmqLbtMMraAv5Gyqn/p/bTPP7fZxSvBzsNDES
translated_by: К. Кирдан
preview: assets/images/previews/Blade_Runner_2049_Rain_Men_Jared_Leto.jpg
math: true
---
_(Возможно, самый важный пост этой цепочки.)_

***Аннотация:*** _Некоторые ценности хуже адаптированы к «крупнейшим вариантам будущего»[^1] по сравнению с другими (см. мой [предыдущий пост](jim-buhler-the-grabby-values-selection-thesis.html)), в том смысле, что они могут ограничивать колонизацию космоса, делая эти ценности менее конкурентоспособными в гонке за распространение по космосу. Предпочтение к уменьшению страданий — это один из примеров предпочтений, которые выглядят особенно неадаптивными и подверженными отсеиванию. Это заставляет агентов, обеспокоенных страданиями, выбирать между предотвращением страданий и увеличением своей способности создавать больше того, что они ценят позитивно. В то время как те, кто не заботится о страданиях, не сталкиваются с этой дилеммой и могут сосредоточиться на оптимизации того, что они ценят позитивно, не беспокоясь о том, какие страдания они могут (не)намеренно причинять. Поэтому при прочих равных мы должны ожидать, что наиболее «[жадные](https://grabbyaliens.com/)» цивилизации/агенты, включая человечество (если оно станет жадным), будут проявлять относительно низкий уровень сострадания. Назовем это «проклятием колонистов, фокусирующихся на плюсах». В этом посте я подробно объясняю эту динамику на примере. Затем я утверждаю, что чем сильнее эта динамика (по сравнению с другими конкурирующими факторами), тем более приоритетным мы должны считать уменьшение [s-рисков](https://forum.effectivealtruism.org/topics/s-risk) по сравнению с уменьшением других долгосрочных рисков, и тем скорее мы должны этим заняться._

## Гуманные ценности, позитивные утилитаристы и штраф за отрицательные ценности

Рассмотрим концепцию «штрафа за отрицательную ценность»: это (субъективное) количество отрицательной ценности, за которую агент (субъект) будет ответственен, чтобы достичь максимального (субъективного) количества положительной ценности. Изложенная ниже история поможет сделать эту идею более интуитивно понятной.

Предположим, что есть два типа агентов:

- те, кто придерживается «гуманных ценностей» («гуманисты»), придавая отрицательную ценность страданиям и положительную ценность таким вещам, как удовольствие (_например, классические [утилитаристы](https://ru.wikipedia.org/wiki/%D0%A3%D1%82%D0%B8%D0%BB%D0%B8%D1%82%D0%B0%D1%80%D0%B8%D0%B7%D0%BC) — прим. пер._);
- «позитивные утилитаристы», которые придают положительную ценность вещам вроде удовольствия, но ничему не придают отрицательной ценности.<br><br>

Эти две группы конкурируют за контроль над их общей планетой, солнечной системой, световым конусом или чем-то еще.

По оценкам гуманистов, они могут колонизировать максимум некоторое количество звезд и создать максимум некоторое количество единиц положительной ценности. Однако они также понимают, что повышение способности их цивилизации создавать положительную ценность одновременно увеличивает [s-риски](https://forum.effectivealtruism.org/topics/s-risk) (в абсолютной величине)[^2]. Таким образом, они сталкиваются с дилеммой между максимизацией положительной ценности и предотвращением страданий, что мотивирует их быть осторожными в вопросе колонизации космоса. Если бы они просто оптимизировали создание большего числа положительной ценности, не беспокоясь о страданиях, за которые они могут (напрямую или косвенно) стать ответственными, то, по их оценкам, на каждые $10$ единиц положительной ценности они причиняли бы $x$ единиц страданий[^3]. Это и есть штраф за отрицательную ценность для гуманистов: $x/10$ (это пропорция, и чем она выше — тем выше штраф).

Позитивные утилитаристы, напротив, не беспокоятся о страданиях, за которые они могут быть ответственными. Они не сталкиваются с той дилеммой, с которой сталкиваются гуманисты, и у них нет стимула быть такими осторожными, как гуманисты. Они могут сразу же начать колонизировать как можно больше звездных систем, чтобы в итоге заполнить их положительной ценностью, не беспокоясь о страданиях. Штраф за отрицательную ценность у позитивных утилитаристов равен нулю.

<img src="assets/images/articles/Blade_Runner_2049_Rain_Men_Jared_Leto_533178_1600x1200.jpg"/>
Изображение 1: Ниандер Уоллес, персонаж из фильма «Бегущий по лезвию 2049», может считаться особенно паршивым позитивным утилитаристом[^4].<br>

Поскольку у гуманистов более высокий штраф за отрицательную ценность (что заставляет их быть осторожнее), их ценности менее «жадные», чем у позитивных утилитаристов. В то время как позитивные утилитаристы спокойно могут распространяться, не боясь отрицательных последствий, гуманисты захотят потратить какое-то количество времени и ресурсов на то, чтобы подумать, как избежать слишком большого количества страданий в процессе колонизации космоса (и стоит ли вообще его колонизировать), поскольку страдания уменьшают для них общую полезность. Согласно [тезису об отборе жадных ценностей](jim-buhler-the-grabby-values-selection-thesis.html) это означает, что при прочих равных мы должны ожидать, что в гонке по колонизации космоса будут отбираться ценности, похожие на ценности позитивных утилитаристов, а не на ценности гуманистов[^5]. Очевидно, что если бы речь шла о системах ценностей, которые уделяют еще больше внимания уменьшению страданий (_например, строгий [негативный утилитаризм](what-is-negative-utilitarianism.html) — прим. пер._), чем у гуманистов, они вытеснялись бы еще сильнее. В этом и заключается «**проклятие колонистов, фокусирующихся на плюсах**» (сокр. UCC — Upside-focused Colonist Curse)[^6].

Понятие штрафа за отрицательную ценность чем-то похоже на [налог на согласование](https://www.lesswrong.com/tag/alignment-tax): чем он больше, тем сложнее выиграть в гонке. Ситуация, в которой находятся позитивные утилитаристы и гуманисты в рассказанной мной истории, аналогична ситуации с конкуренцией между сторонниками наращивания возможностей ИИ и сторонниками обеспечения безопасности ИИ, которая сложилась в реальном мире.

Этот эффект UCC-отбора может происходить как внутри одной цивилизации (внутрицивилизационный отбор, когда гуманисты и позитивные утилитаристы принадлежат к одной цивилизации), так и между различными цивилизациями (межцивилизационный отбор, когда гуманисты и позитивные утилитаристы представляют конкурирующие цивилизации)[^7].

## Ответы на очевидные возражения

_(В основном актуально в межцивилизационном контексте.)_ ***Но почему агенты, беспокоящиеся о страданиях, просто не сделают колонизацию космоса приоритетной, а о том, как наилучшим образом максимизировать свою функцию полезности — подумают позже, чтобы не проиграть гонку позитивным утилитаристам?***

Это то, что в [моем предыдущем посте](jim-buhler-the-grabby-values-selection-thesis.html) названо «аргументом о конвергентной превентивной колонизации». Мы увидим, что этот аргумент плохо подходит к приведенному здесь примеру. (См. [Shulman 2012](https://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html) для примера, где этот аргумент работает получше.) Действительно, кажется, что гуманисты могли бы отложить некоторые действия. Например, размышления о том, как создавать счастливые умы без значительных [сопутствующих страданий](https://centerforreducingsuffering.org/research/a-typology-of-s-risks/#Incidental_s-risks), и, следовательно, само их создание. Однако...
 
- это зависит от того, достаточно ли они терпеливы.
- они не могут так же отложить уменьшение всех видов s-рисков. Идея приоритизировать колонизацию космоса и отложить предотвращение страданий может быть очень схожа с идеей приоритизировать наращивание возможностей ИИ, отложив обеспечение безопасности ИИ. Если приоритет будет отдан первому, для второго может оказаться уже слишком поздно. Вот несколько примеров:
  - они не могут откладывать предотвращение отрицательной ценности, исходящей от конфликтов с другими агентами (см. [Clifton 2019](https://www.lesswrong.com/posts/DbuCdEbkh4wL5cjJ5/preface-to-clr-s-research-agenda-on-cooperation-conflict-and); [Sandberg 2021](https://www.youtube.com/watch?v=jMouMl7RHk0)). Им нужно подумать об этом заранее, до встречи с этими другими агентами, поэтому они потратят на это время/ресурсы, вместо того чтобы быстрее начинать/проводить колонизацию.
  - они также не могут откладывать предотвращение страданий, связанных с подпрограммами/симуляциями (см. [Tomasik 2015](https://longtermrisk.org/risks-of-astronomical-future-suffering)), которые могут оказаться инструментально полезными в процессе колонизации. Им нужно обдумать это заранее.
  - гуманисты должны больше беспокоиться (по сравнению с позитивными утилитаристами) о возможности дрейфа или взлома их ценностей (напр., злонамеренными акторами), поскольку это может привести к чрезвычайно катастрофическим «близким промахам» \[near miss\] для них (см. [Tomasik 2015](https://reducing-suffering.org/near-miss/); [Leech 2022](https://forum.effectivealtruism.org/posts/j4G5Gqxa6JmbbQYzX/mediocre-ai-safety-as-existential-risk#Risks)). Гуманисты рискуют сильнее, чем позитивные утилитаристы, поэтому они захотят потратить время и ресурсы на предотвращение таких потерь на ранних этапах.
- в нашем нынешнем мире беспокойство о страданиях, по-видимому, коррелирует со скептицизмом и неопределенностью насчет ценности колонизации космоса. Эта корреляция настолько сильна, что может быть трудно отсеять второе без отсеивания первого. И хотя это может показаться специфичным именно для современных людей, это, похоже, тенденция, которая в какой-то степени обобщается также на наших потомков и инопланетян.
- беспокойство гуманистов о страданиях может быть дополнительно асимметричным по оси [действия-упущения](https://ru.wikipedia.org/wiki/%D0%9D%D0%B5%D0%B4%D0%BE%D0%BE%D1%86%D0%B5%D0%BD%D0%BA%D0%B0_%D0%B1%D0%B5%D0%B7%D0%B4%D0%B5%D0%B9%D1%81%D1%82%D0%B2%D0%B8%D1%8F), из-за чего они больше обеспокоены теми страданиями, которые они могут причинить, чем теми, которые они могут предотвратить, — что делает их менее активными сторонниками колонизации космоса.<br><br>

Обратите внимание, что все эти пункты независимы. Не нужно соглашаться со всеми из них, чтобы согласиться с моим утверждением в целом.


_(В основном актуально во внутрицивилизационном контексте)_ ***Но почему бы агентам, обеспокоенным страданиями, не попытаться присоединиться к позитивным утилитаристам в их усилиях по колонизации космоса и не настаивать на большей осторожности, вместо того чтобы пытаться их победить?***

В самом деле, они скорее всего попробуют это сделать. Однако мы должны ожидать, что их постепенно будут вытеснять, поскольку они замедляют проект позитивных утилитаристов по колонизации космоса. Немногие гуманисты, которым удастся избежать дрейфа ценностей и не быть вытесненными из группы позитивных утилитаристов, проводящих колонизацию, — это те, кто довольствуется происходящим и не слишком сильно настаивает на осторожности.

***Если проклятие колонистов, фокусирующихся на плюсах (UCC), реально, то реален и отбор против всего, что отличается от позиции «давайте захватывать как можно больше ресурсов, остальное неважно».***

Да, (технически) это так. Однако отбор против беспокойства о страданиях / отрицательных ценностях кажется более значительным, чем отбор против ценностей как у позитивных утилитаристов. На самом деле, Карл Шульман ([2012](https://reflectivedisequilibrium.blogspot.com/2012/09/spreading-happiness-to-stars-seems.html)) утверждает, что в последнем случае отбор очень слаб. И хотя некоторые из его предположений, по моему мнению, плохо подкреплены, я думаю, что приведенные им соображения довольно значимы. Тогда как отрицательная ценность чего-либо похожа на заметное ограничение, _особенно если_ речь об отрицательной ценности страданий, которые могут происходить по многим причинам. Поэтому системы ценностей, требующие минимизации страданий, выглядят более склонными к тому, чтобы отсеиваться.

<img src="https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/5Nm93x8GynXPhZpYe/ioiuwmbmdlgbvwfwj3od">

Схема 1: Грубая иллюстрация моих интуиций о том, насколько жадны различные моральные предпочтения.

## Возможные выводы

Я думаю, что показал, что крупнейшие варианты будущего коррелируют с незначительным беспокойством о страданиях (т. е. UCC действительно имеет место быть), — _при прочих равных условиях_. Очевидно, что вряд ли _прочие условия_ на деле _равны_. Похоже, что степень, в которой UCC является решающим фактором по сравнению с другими динамиками, в основном зависит от силы (более широкого) [эффекта «отбора жадных ценностей»](jim-buhler-the-grabby-values-selection-thesis.html), который я кратко и грубо попытался оценить в [этом разделе](jim-buhler-the-grabby-values-selection-thesis.html#Насколько_силен_эффект_отбора) предыдущего поста.

Однако в данном разделе я хочу обрисовать некоторые выводы из UCC, исходя из предположения, что эта динамика является более-менее решающим фактором.

### Ценности инопланетян могут быть не хуже, чем у наших потомков, что снижает важность (некоторых путей) снижения x-рисков

Браунер и Гроссе-Хольц ([2018, часть 2.1](https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive?fbclid=IwAR2Si8qdOEqXdPujDfv6gDGLaTdevs4Tb_CALW0D2MHUC4Ot9evEAoem3Gw#2_1_Whether__post__humans_colonizing_space_is_good_or_bad__space_colonization_by_other_agents_seems_worse)) утверждают, что «\[независимо от того,\] будет ли колонизация космоса (пост-)людьми хорошей или плохой, колонизация космоса другими агентами представляется хуже», и что это аргумент в пользу приоритизации снижения x-рисков.

Как они объясняют:

> Если человечество вымрет, не колонизировав космос, другие существа на Земле, вероятно, выживут. В течение сотен миллионов лет, оставшихся Земле, эти существа могут эволюционировать в нечеловеческую технологическую цивилизацию, и в конечном итоге колонизировать космос. Точно так же инопланетяне (которые уже могут существовать или появятся в будущем) могут колонизировать (большую часть) нашего уголка Вселенной, если этого не сделает человечество.

Затем они утверждают, что «_наши предпочтения-после-размышлений скорее всего в большей степени совпадают с таковыми у (пост-)человеческой цивилизации, чем с таковыми у альтернативных цивилизаций_». Хотя это, вероятно, верно в маленьких вариантах будущего, в которых цивилизации не слишком жадны, [тезис об отборе жадных ценностей](jim-buhler-the-grabby-values-selection-thesis.html) предполагает, что самые жадные цивилизации сходятся к ценностям, которые особенно способствуют экспансии. Более конкретно, исходя из UCC мы должны ожидать, что они будут мало обеспокоены страданиями, независимо от того, люди это или инопланетяне. Поэтому похоже, что утверждение Браунера и Гроссе-Хольца неверно для крупнейших вариантов будущего человечества, при условии, что:

- сентиентность является конвергентной особенностью разных цивилизаций, и нет веских причин предполагать, что жадные инопланетяне менее склонны ценить вещи вроде удовольствия, чем жадные люди;
- мы — люди, рассматривающие этот аргумент — _беспристрастны_ (см. [MacAskill 2019](https://forum.effectivealtruism.org/posts/9wYa8BqSTMcx9j2tK/defining-effective-altruism)), мы не делаем произвольных различий, вроде разной отрицательной или положительной ценности чего-либо в зависимости от того, вызывается оно людьми или другими агентами.<br><br>

Хотя я на 100% поддерживаю пункт №2[^8], пункт №1 не является само собой разумеющимся, так что приведенный выше аргумент Браунера и Гроссе-Хольца ([2018, часть 2.1](https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive?fbclid=IwAR2Si8qdOEqXdPujDfv6gDGLaTdevs4Tb_CALW0D2MHUC4Ot9evEAoem3Gw#2_1_Whether__post__humans_colonizing_space_is_good_or_bad__space_colonization_by_other_agents_seems_worse)) по-прежнему имеет некоторую силу. При прочих равных жадные люди могут быть несколько более склонны распространять вещи вроде удовольствия, чем средняя жадная инопланетная цивилизация.

Более того, даже если предположить, что у самых жадных инопланетян и самых жадных людей очень похожие ценности, можно утверждать, что все же предпочтительнее не ждать, пока инопланетяне распространят что-то потенциально ценное, что могло бы раньше них распространить человечество (при условии, что ожидаемая полезность будущего влияния человечества на вселенную положительна). Важность этого соображения зависит от того, насколько в среднем будет отложена колонизация нашего уголка Вселенной, если её осуществит другая цивилизация вместо человечества.

Однако я считаю, что UCC может существенно уменьшить важность снижения x-рисков. Я ожидаю, что жадные люди и жадные инопланетяне все же со значительной вероятностью создадут вещи, которые будут схожими по своей отрицательной или положительной ценности, что ставит под сомнение довольно значимый аргумент в пользу приоритизации (некоторых)[^9] x-рисков (см. [Guttman 2022](https://forum.effectivealtruism.org/posts/zDJpYMtewowKXkHyG/alien-counterfactuals), [Tomasik 2015](https://longtermrisk.org/risks-of-astronomical-future-suffering/#What_if_human_colonization_is_more_humane_than_ET_colonization); [Aird 2020](https://forum.effectivealtruism.org/posts/wicAtfihz2JmPRgez/crucial-questions-for-longtermists)).

### Нынешние лонгтермисты могут иметь сильное сравнительное преимущество в снижении s-рисков

Этот пункт довольно очевиден. Мы утверждали, что в мирах, где ставки выше всего, отбираются агенты, фокусирующиеся на плюсах, так что мы должны — при прочих равных — ожидать (или действовать так, будто ожидаем), что те, кто будет контролировать наш уголок Вселенной, будут больше заботиться о вещах вроде удовольствия, чем о страданиях. Это означает, что хотя мы можем — в некоторой степени — рассчитывать на то, что наши потомки и/или инопланетяне создадут то, что мы (или наши предпочтения-после-размышлений) нашли бы положительно ценным, мы не можем рассчитывать на то, что они будут избегать [s-рисков](https://forum.effectivealtruism.org/topics/s-risk). В то время как s-риски уже и так сильно недооцениваются нынешними людьми (см. [Baumann 2022](https://forum.effectivealtruism.org/posts/XyCLLYkBCPw44jpmQ/new-book-on-s-risks); [Gloor 2023](https://forum.effectivealtruism.org/posts/8yaQ6i3oaFLprsFyb/ai-alignment-researchers-may-have-a-comparative-advantage-in#Alignment_researchers_have_a_comparative_advantage_in_reducing_s_risks)), эта недооценённость может ещё и значительно вырасти в далёком будущем.

И я сомневаюсь, что кто-либо из лонгтермистов считает важность и осуществимость снижения s-рисков настолько низкими (см., напр., [Gloor 2023](https://forum.effectivealtruism.org/posts/8yaQ6i3oaFLprsFyb/ai-alignment-researchers-may-have-a-comparative-advantage-in#Alignment_researchers_have_a_comparative_advantage_in_reducing_s_risks) для аргументов против этого), чтобы считать, что можно ничего не предпринимать насчет перспективы увеличения игнорируемости этих рисков в будущем.

Поэтому лонгтермисты могут захотеть увеличить степень приоритетности снижения s-рисков (устойчивым образом), например, путем разработки эффективных мер предосторожности против [близких промахов](https://reducing-suffering.org/near-miss/), чтобы избавить ИИ, фокусирующихся на плюсах, от высоких издержек предотвращения страданий.

К слову, я ещё не думал всерьёз о том, может ли — и как именно — соображение о проклятии колонистов, фокусирующихся на плюсах (UCC), повлиять на приоритеты исследователей s-рисков (см., напр., исследовательские повестки [CLR](https://longtermrisk.org/research-agenda) и [CRS](https://centerforreducingsuffering.org/open-research-questions/)), но это может быть многообещающей темой для будущих исследований.

## Заключение

В той мере, в какой UCC является вероятным следствием [тезиса об отборе жадных ценностей](jim-buhler-the-grabby-values-selection-thesis.html), оно кажется почти тривиально верным.

Тем не менее, я в глубокой неопределенности насчет того, насколько оно значимо и, следовательно, насчет того, является ли это веским аргументом в пользу приоритизации снижения s-рисков по сравнению со снижением других долгосрочных рисков. Есть ли какое-то важное соображение, которое я упускаю? Например, есть ли причины полагать, что агенты/цивилизации, беспокоящиеся о страданиях, могут — на самом деле — выигрывать в отборе и оказываться среди наиболее жадных? Как мы могли бы провести оценки Ферми для силы эффекта отбора? Буду рад услышать, что вы думаете!

Кроме того, если UCC действительно имеет решающее значение, следует ли из этого что-то ещё, кроме того, что современным лонгтермистам, возможно, стоит приоритизировать уменьшение s-рисков? Что вы думаете?

Здесь нужно больше исследований. Пожалуйста, свяжитесь со мной, если вам интересно поработать над этими вопросами!

## Благодарности

Спасибо Робину Хэнсону, Максиме Рише и Энтони ДиДжованни за глубокие обсуждения близких тем. Спасибо Антонину Брои за ценные комментарии к ранней версии статьи. Спасибо Майклу Сент-Джулсу, Чарли Гутману, Джеймсу Фэвиллу, Джоджо Ли, Юэну Маклину, Тимоти Чану и Матиссу Апинису за полезные комментарии к поздним версиям статьи. Мне также принесла пользу возможность представить мою работу по этой теме на ретрите, организованном EA Oxford (спасибо организаторам и участникам ретрита, участвовавшим в дискуссии).

Большая часть моей работы над этой цепочкой до сих пор финансировалась [Existential Risk Alliance](https://erafellowship.org/).

Все допущения/утверждения/упущения являются моими.

## Сноски

[^1]: Т. е. в вариантах будущего с наибольшим потенциалом в плане того, что мы можем создать / на что можем повлиять (см. [MacAskill 2022, Глава 1](https://forum.effectivealtruism.org/posts/rdigzNQqDgiou5AmZ/what-we-owe-the-future-chapter-1#The_Future_Is_Big)).

[^2]:
    Похоже, что в нашем мире это верно в общем случае, даже если заменить s-риски на что-то другое, что для агента отрицательно ценно. В общем случае нельзя увеличить свою способность создавать положительные ценности без немалого увеличения риска того, что (прямо или косвенно) будет создано больше того, что имеет отрицательную ценность (т. е. если ты считаешь что-то отрицательно ценным, у тебя будет ненулевой штраф за отрицательную ценность). Например, скажем, Боб положительно ценит здания и отрицательно ценит деревья. Может показаться, что колонизация космоса для создания множества зданий не несет значительного риска создания большего числа деревьев. Но есть как минимум две причины полагать, что у Боба есть ощутимый штраф за отрицательную ценность. Во-первых, ошибки или дрейф ценностей могут привести к катастрофическому близкому промаху (см. [Tomasik 2019](https://reducing-suffering.org/near-miss/), [Leech 2022](https://forum.effectivealtruism.org/posts/j4G5Gqxa6JmbbQYzX/mediocre-ai-safety-as-existential-risk#Risks)). Во-вторых, Боб может и не быть единственным влиятельным агентом поблизости, так что его экспансия прямо увеличивает вероятность конфликта с другими агентами или цивилизациями. Такой конфликт может стать катастрофическим, приведя к созданию того, что вовлеченные в него агенты считают отрицательным (см. [Clifton 2019](https://www.lesswrong.com/posts/DbuCdEbkh4wL5cjJ5/preface-to-clr-s-research-agenda-on-cooperation-conflict-and); [Sandberg 2021](https://www.youtube.com/watch?v=jMouMl7RHk0)).
    
    И хотя экспансия Боба связана с немалым риском создания деревьев, риск того, что экспансия гуманистов (косвенно или случайно) вызовет страдания, может быть выше — по трем причинам. Во-первых, [негативное подкрепление может быть полезным](https://reducing-suffering.org/what-are-suffering-subroutines/) для выполнения многих задач (в отличие от создания деревьев). Во-вторых, катастрофический близкий промах выглядит намного более вероятным для гуманных ценностей, чем для ценностей Боба, так как в пространстве всего, что можно создать, источники положительной ценности, такие как удовольствие, гораздо ближе к страданиям, чем деревья к зданиям. В-третьих, технологии, позволяющие создавать больше положительной ценности, могут усиливать и возможных садистических/мстительных акторов, жаждущих создавать страдания.
    
    Одно из контр-соображений заключается в том, что чем больше гуманисты колонизируют космос, тем больше они могут уменьшать возможные неантропогенные страдания (см. [Vinding & Baumann 2021](https://centerforreducingsuffering.org/s-risk-impact-distribution-is-double-tailed/)). Однако создание страданий выглядит гораздо более простым, чем уменьшение страданий, приносимых другими цивилизациями, так что мы все равно должны ожидать, что экспансия гуманистов в целом увеличит s-риски.

[^3]: Где $x > 0$ и метрика для оценки положительных и отрицательных ценностей построена так, что 1 единица положительной ценности субъективно компенсирует 1 единицу страданий.

[^4]: «_Ты любишь боль. Боль напоминает тебе, что радость, которую ты чувствовал, была настоящей. Тогда будет больше радости! Не бойся. \[...\] Все достижения цивилизации зиждятся на использовании расходуемой рабочей силы. Но я могу создать лишь столько._» Спасибо Юэну МакЛину за то, что он привлек мое внимание к этому интересному вымышленному примеру.

[^5]: И если существуют позитивные утилитаристы наподобие Уоллеса (которые также ценят увеличение s-рисков/страданий, происходящее из-за их экспансии/прогресса, потому что «_это часть жизни!_» или «_существование ада заставляет нас больше ценить рай!_» или еще что), мы можем ожидать, что они будут отбираться еще сильнее. Страдания могут стать не просто тем, о чем никто не беспокоится, но и тем, что ценится позитивно.

[^6]: UCC отчасти напоминает общие потенциальные динамики, уже описанные другими авторами. В [моем посте №2 из этой серии](jim-buhler-what-the-moral-truth-might-be-makes-no-difference.html): «Ник Бостром ([2004](https://nickbostrom.com/fut/evolution)) исследует "_сценарии, в которых свободное эволюционное развитие, продолжая производить сложные и интеллектуальные формы организации, приводит к постепенному исчезновению всех форм существования, которые нас заботят_". Пол Кристиано ([2019](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)) описывает сценарий, в котором "_ML-обучение \[...\] порождает “жадные” паттерны, стремящиеся расширить своё влияние_". Аллан Дафо ([2019](https://docs.google.com/document/d/1B77VWaXG-u34nSRFKV14pJNHJHHb6sa5zJ08J70CVVA/edit); [2020](https://forum.effectivealtruism.org/posts/42reWndoTEhFqu6T8/ai-governance-opportunity-and-theory-of-impact#Value_Erosion_through_Competition)) ввёл термин "_эрозия ценностей_", чтобы описать динамику, где "_так же как дилемма между безопасностью и производительностью при интенсивной конкуренции заставляет принимать решения в ущерб безопасности, так и дилемма между любой человеческой ценностью и конкурентоспособностью может стимулировать принятие решений в ущерб этой ценности_".» Мое утверждение, однако, более слабое и менее спорное, чем у них. Описываемая мной динамика заключается в том, что отсеиванию подвергается забота именно об отрицательной ценности (т. е. один конкретный аспект наших моральных предпочтений), тогда как в динамиках, описанных ими, предполагается постепенное исчезновение наших моральных предпочтений в целом. Мое утверждение слабее, потому что сострадание явно менее адаптировано к гонкам по колонизации космоса, чем вещи вроде заботы о «процветании человечества». К примеру, предпочтения заполнить как можно большее количество звездных систем положительными ценностями гораздо более конкурентоспособны в этом контексте, чем предпочтения уменьшить s-риски. Ясно, что первые тоже могут подвергаться отсеиванию, потому что они могут оказаться менее адаптивными, чем что-нибудь вроде предпочтения распространяться как можно больше ради самого распространения. Но беспокойство о страданиях гораздо менее конкурентоспособно, поэтому гораздо вероятнее будет подвергаться отсеиванию. Я немного подробнее разбираю это в ответе на третье возражение в следующем разделе.

[^7]: См. больше подробностей о внутрицивилизационном и межцивилизационном отборе в моем [предыдущем посте](jim-buhler-the-grabby-values-selection-thesis.html).

[^8]: Браунер и Гроссе-Хотц ([2018, часть 2.1](https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive?fbclid=IwAR2Si8qdOEqXdPujDfv6gDGLaTdevs4Tb_CALW0D2MHUC4Ot9evEAoem3Gw#2_1_Whether__post__humans_colonizing_space_is_good_or_bad__space_colonization_by_other_agents_seems_worse)) предполагают, что мы должны считать чьи-то ценности «хуже», если они не «_зависят от каких-то аспектов человеческого бытия, таких как человеческая культура или биологическое устройство человеческого мозга_». Я абсолютно не согласен с этим и оценил сделанную ими сноску, в которой говорится, что «_несколько человек, прочитавших раннюю версию \[их\] статьи, прокомментировали, что они могут себе представить, что их предпочтения-после-размышлений не будут зависеть от специфических для людей факторов_».

[^9]: Только «некоторых»? Действительно, в некоторых случаях это может не относиться к риску несогласованности AGI, поскольку несогласованный AGI может не только помешать человечеству колонизировать космос, но и захватывать ресурсы и препятствовать появлению других цивилизаций и/или замедлять жадных инопланетян, «стоя на их пути» и заставляя их идти на моральные компромиссы, в то время как они могли бы просто заполнить наш уголок вселенной чем-то, что они положительно ценят, если бы этого несогласованного ИИ не было.
