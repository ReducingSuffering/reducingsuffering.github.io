---
layout: article
title: Список источников против жесткого взлета ИИ
authors:
  - Магнус Виндинг
original: https://magnusvinding.com/2017/12/16/a-contra-ai-foom-reading-list/
translated_by: К. Кирдан
original_date:
  - "2017"
  - "2023.11"
preview: assets/images/previews/automation-7411686-.jpg
preview_here: true
---
Мне кажется, что есть значительная асимметрия в том внимании, которое уделяется аргументам в пользу правдоподобности сценариев [жесткого взлета](https://old-wiki.lesswrong.com/wiki/AI_takeoff#Hard_takeoff) / FOOM искусственного интеллекта (т. е. когда ИИ становится весьма могущественным в течение минут, дней или месяцев, что приводит к [сингулярности](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D1%85%D0%BD%D0%BE%D0%BB%D0%BE%D0%B3%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%81%D0%B8%D0%BD%D0%B3%D1%83%D0%BB%D1%8F%D1%80%D0%BD%D0%BE%D1%81%D1%82%D1%8C) с потенциально катастрофическими последствиями — прим. пер.) по сравнению с вниманием, уделяемым контраргументам к этим сценариям. Это не так уж странно, учитывая наличие полноценных широко разрекламированных книг, подчеркивающих аргументы этой стороны, таких как «‎[Искусственный интеллект](https://www.mann-ivanov-ferber.ru/catalog/product/iskusstvennyj-intellekt/)» Ника Бострома и «‎[Последнее изобретение человечества](https://alpinabook.ru/catalog/book-poslednee-izobretenie-chelovechestva/)» Джеймса Баррата, в то время как, похоже, нет подобной книги, которая подчеркивала бы контраргументы. А люди, которые скептически относятся к сценариям жесткого взлета, и которые думают, что есть вещи поважнее — будут склонны, конечно, писать книги об этих других более важных по их мнению вещах. В итоге, аргументам на тему взлета ИИ они посвящают только эссе или несколько постов в блоге, а не полноценные книги. Цель данного списка источников состоит в том, чтобы попытаться немного исправить эту асимметрию, указав людям на некоторые из тех постов и эссе, как и других ресурсов, которые представляют причины скептически относиться к сценарию жесткого взлета.

Я думаю, что важно вынести эти аргументы на обсуждение, поскольку, как мне представляется, в противном случае мы рискуем получить слишком однобокий взгляд на эту проблему и, не в последнюю очередь, упустить из виду другие вещи, на которых, возможно, стоило бы сосредоточиться.

Я должен отметить, что не всегда согласен со всеми утверждениями и аргументами, которые приводятся в этих источниках. Например, Стивен Пинкер выдвинул несколько плохих аргументов, из-за которых скептицизм в отношении жесткого взлета выглядит невежеством. Тем не менее я все равно считаю, что в каждой из приведенных ниже позиций есть хоть какие-то положительные моменты. Я хотел бы также отметить, что не все из следующих авторов считают жесткий взлет крайне маловероятным — просто считают более вероятными другие сценарии.

---
**Робин Хэнсон:**

- [Some Skepticism](http://mason.gmu.edu/~rhanson/vc.html#hanson)
- [_The Hanson-Yudkowsky AI-Foom Debate_](http://intelligence.org/files/AIFoomDebate.pdf) (в соавторстве с Элиезером Юдковским)
- [Yudkowsky vs Hanson — Singularity Debate](https://www.youtube.com/watch?v=TuXl-iidnFY) (видео)
- [Setting The Stage](http://www.overcomingbias.com/2008/11/setting-the-sta.html)
- [AI Go Foom](http://www.overcomingbias.com/2008/11/ai-go-foom.html)
- [Emulations Go Foom](http://www.overcomingbias.com/2008/11/emulations-go-f.html)
- [Wrapping Up](http://www.overcomingbias.com/2008/12/wrapping-up.html)
- [Two Visions Of Heritage](http://www.overcomingbias.com/2008/12/two-visions-of.html)
- [Distrusting Drama](http://www.overcomingbias.com/2008/12/types-of-distru.html)
- [What Core Argument?](http://www.overcomingbias.com/2008/12/what-core-argument.html)
- [Is The City-ularity Near?](http://www.overcomingbias.com/2010/02/is-the-city-ularity-near.html)
- [The Betterness Explosion](http://www.overcomingbias.com/2011/06/the-betterness-explosion.html)
- [Meet the new conflict, same as the old conflict: Comment on David Chalmers “The Singularity: A Philosophical Analysis”](http://mason.gmu.edu/~rhanson/ChalmersReply.html)
- [Debating Yudkowsky](http://www.overcomingbias.com/2011/07/debating-yudkowsky.html)
- [When Is “Soon”?](http://www.overcomingbias.com/2012/06/when-is-soon.html)
- [A History Of Foom](http://www.overcomingbias.com/2013/01/a-history-of-foom.html)
- [Foom Debate, Again](http://www.overcomingbias.com/2013/02/foom-debate-again.html)
- [I Still Don’t Get Foom](http://www.overcomingbias.com/2014/07/30855.html)
- [Irreducible Detail](http://www.overcomingbias.com/2014/07/limits-on-generality.html)
- [This Time Isn’t Different](http://www.overcomingbias.com/2014/11/this-time-isnt-different.html)
- [How Different AGI Software?](http://www.overcomingbias.com/2016/03/how-different-agi-software.html)
- [Hanson on intelligence explosion, from Age of Em](http://lukemuehlhauser.com/hanson-on-intelligence-explosion-from-age-of-em/)
- [Brains Simpler Than Brain Cells?](http://www.overcomingbias.com/2016/11/brains-simpler-than-brain-cells.html)
- [This AI Boom Will Also Bust](http://www.overcomingbias.com/2016/12/this-ai-boom-will-also-bust.html)
- [Foom Justifies AI Risk Efforts Now](https://www.overcomingbias.com/2017/08/foom-justifies-ai-risk-efforts-now.html)
- [How Deviant Recent AI Progress Lumpiness?](https://www.overcomingbias.com/2018/03/how-deviant-recent-ai-progress-lumpiness.html)
- [Robin Hanson on AI Skepticism](https://www.youtube.com/watch?v=QbHzxHsnAtk&t=1s) (видео)
- [How Lumpy AI Services?](http://www.overcomingbias.com/2019/02/how-lumpy-ai-services.html)
- [Robin Hanson on AI Takeoff Scenarios – AI Go Foom?](https://www.youtube.com/watch?v=qk3bQrSfUzs) (видео) 
- [Decentralized Approaches to AI Presentations](https://www.youtube.com/watch?time_continue=3533&v=pClSjljMKeA) (видео; представлены также Эрик Дрекслер и Марк Миллер)
- [Conversation with Robin Hanson](https://aiimpacts.org/conversation-with-robin-hanson/)
- [Foom Update](https://www.overcomingbias.com/2022/05/foom-update.html)
- [Why Not Wait On AI Risk?](https://www.overcomingbias.com/2022/06/why-not-wait-on-ai-risk.html)
- [AGI Is Sacred](https://www.overcomingbias.com/2022/08/agi-is-sacred.html)
- [Robin Hanson on Predicting the Future of Artificial Intelligence](https://www.youtube.com/watch?v=hOuG018Fsig&ab_channel=FutureofLifeInstitute) (видео)
- [AI Risk, Again](https://www.overcomingbias.com/p/ai-risk-again)
- [Waiting for the Betterness Explosion](https://www.youtube.com/watch?v=U1RZknHchi0&ab_channel=CSPI) (видео)
- [What Are Reasonable AI Fears?](https://quillette.com/2023/04/14/what-are-reasonable-ai-fears/)
- [The Optimistic Hansonian Singularity](https://transhumanaxiology.substack.com/p/the-optimistic-hansonian-singularity#details) (подкаст)
<br><br>

**Рамез Наам:**

- [Top Five Reasons ‘The Singularity’ Is A Misnomer](https://web.archive.org/web/20110316035712/http://hplusmagazine.com/2010/11/11/top-five-reasons-singularity-misnomer/)
- [The Singularity is Further Than it Appears](http://rameznaam.com/2015/05/12/the-singularity-is-further-than-it-appears/) <br> [Сингулярность дальше, чем кажется](https://vk.com/@posthuman_identity-ramez-naam-singularity-is-further-than-it-appears), пер. Posthuman Identity
- [Why AIs Won’t Ascend in the Blink of an Eye – Some Math](http://www.antipope.org/charlie/blog-static/2014/02/why-ais-wont-ascend-in-blink-of-an-eye.html)
<br><br>

**Бен Гарфинкель:**

- [How sure are we about this AI stuff?](https://www.youtube.com/watch?v=E8PGcoLDjVk) (видео)
- [Ben Garfinkel on scrutinising classic AI risk arguments](https://80000hours.org/podcast/episodes/ben-garfinkel-classic-ai-risk-arguments/)
- [Does economic history point toward a singularity?](https://drive.google.com/file/d/11QnrnXpoQYYMtstDJxcuW7hV7zsxr5ZV/view)
<br><br>

**Тим Тайлер:**

- [The Intelligence Explosion Is Happening Now](http://alife.co.uk/essays/the_intelligence_explosion_is_happening_now/)
- [The Singularity Is Nonsense](http://alife.co.uk/essays/the_singularity_is_nonsense/)
- [Against the Singularity](http://alife.co.uk/essays/against_the_singularity/) <br> [Против сингулярности](https://spacemorgue.com/against-the-singularity/), пер. Сергея Морякина
<br><br>

**Стивен Пинкер:**

- [We’re told to fear robots. But why do we think they’ll turn on us?](https://www.popsci.com/robot-uprising-enlightenment-now)
- [MIT AGI: AI in the Age of Reason (Steven Pinker)](https://www.youtube.com/watch?v=epQxfSp-rdU) (видео)
- [Steven Pinker and Stuart Russell on the Foundations, Benefits, and Possible Existential Threat of AI](https://www.youtube.com/watch?v=OYx_kfgcfro)
<br><br>

**Эрик Дрекслер:**

- [Reframing Superintelligence: Comprehensive AI Services as General Intelligence](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf)
- [Reframing Superintelligence](https://www.youtube.com/watch?v=MircoV5LKvg) (видео)
<br><br>

**Брайан Томасик:**

- [Artificial Intelligence and Its Implications for Future Suffering](https://foundational-research.org/artificial-intelligence-and-its-implications-for-future-suffering)
- [Artificial general intelligence as a continuation of existing software and societal trends](https://reducing-suffering.org/agi-continues-default-trends/)
<br><br>

**Джейкоб Бакмен:**

- [Recursively Self-Improving AI Is Already Here](https://jacobbuckman.com/2022-09-07-recursively-self-improving-ai-is-already-here/)
- [We Aren’t Close To Creating A Rapidly Self-Improving AI](https://jacobbuckman.substack.com/p/we-arent-close-to-creating-a-rapidly)
<br><br>

**Джефф Хокинс:**

- [The Future of Artificial Intelligence – Up Next](https://www.youtube.com/watch?v=3tgyaeP1lnU) (видео)
- [The Terminator Is Not Coming. The Future Will Thank Us.](https://www.recode.net/2015/3/2/11559576/the-terminator-is-not-coming-the-future-will-thank-us)
<br><br>

**Бен Герцель:**

- [The Singularity Institute’s Scary Idea (and Why I Don’t Buy It)](http://multiverseaccordingtoben.blogspot.dk/2010/10/singularity-institutes-scary-idea-and.html)
- [Superintelligence: fears, promises, and potentials](http://www.kurzweilai.net/superintelligence-fears-promises-and-potentials)
<br><br>

**Дэвид Пирс:**

- [The Biointelligence Explosion](https://www.biointelligence-explosion.com/) ([Extended Abstract](https://www.hedweb.com/intelligence-explosion/index.html))
- [Humans and Intelligent Machines: Co-Evolution, Fusion or Replacement?](https://www.biointelligence-explosion.com/parable.html)
<br><br>

**Дж Сторрс Холл:**

- [Self-improving AI: an Analysis](http://autogeny.org/Hall-Self-improving-AI-an-analysis.pdf)
- [Engineering Utopia](https://web.archive.org/web/20141201201504/http://www.agiri.org/takeoff_hall.pdf)
<br><br>

**Моника Андерсон:**

- [Problem Solved: Unfriendly AI](http://hplusmagazine.com/2010/12/15/problem-solved-unfriendly-ai/)
- [Reduction Considered Harmful](https://web.archive.org/web/20160618065958/http://hplusmagazine.com/2011/03/31/reduction-considered-harmful/)
<br><br>

**Джеймс Фодор:**

- [Critique of Superintelligence](https://forum.effectivealtruism.org/posts/A8ndMGC4FTQq46RRX/criqitue-of-superintelligence-part-1) (ссылка на первый пост из пяти взаимосвязанных)
- [A Critique of AI Takeover Scenarios](https://forum.effectivealtruism.org/posts/j7X8nQ7YvvA7Pi4BX/a-critique-of-ai-takeover-scenarios)
<br><br>

**Дэвид Торстад:**

- [Against the singularity hypothesis](https://philpapers.org/archive/THOATS-5.pdf)
<br><br>

**Теодор Модис:**

- [Why the Singularity Cannot Happen](http://growth-dynamics.com/articles/Singularity.pdf)
<br><br>

**Франсуа Холлет:**

- [The implausibility of intelligence explosion](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec)
<br><br>

**Родни Брукс:**

- [The Seven Deadly Sins of Predicting the Future of AI](https://rodneybrooks.com/the-seven-deadly-sins-of-predicting-the-future-of-ai/)
<br><br>

**Катя Грейс:**

- [Counterarguments to the basic AI x-risk case](https://aiimpacts.org/counterarguments-to-the-basic-ai-x-risk-case/)
<br><br>

**Филипп Агион, Бенджамин Ф. Джонс, & Чарльз И. Джонс:**

- [Artificial Intelligence and Economic Growth](https://web.stanford.edu/~chadj/AI.pdf)
<br><br>

**Алессио Плебе & Пьетро Перконти:**

- [The slowdown hypothesis (extended abstract)](http://singularityhypothesis.blogspot.com/2011/07/slowdown-hypothesis.html)
<br><br>

**Макс Мор:**

- [Singularity Meets Economy](http://mason.gmu.edu/~rhanson/vc.html#more)
<br><br>

**Ричард Лузмор:**

- [The Maverick Nanny with a Dopamine Drip](https://ieet.org/index.php/IEET2/more/loosemore20140724)
<br><br>

**Эрнст Дэвис:**

- [Ethical Guidelines for A Superintelligence](https://cs.nyu.edu/davise/papers/Bostrom.pdf)
<br><br>

**AI Impacts:**

- [Likelihood of discontinuous progress around the development of AGI](https://aiimpacts.org/likelihood-of-discontinuous-progress-around-the-development-of-agi/)
<br><br>

**Себастьян Бенталл:**

- [Don’t Fear the Reaper: Refuting Bostrom’s Superintelligence Argument](https://arxiv.org/pdf/1702.08495.pdf)
<br><br>

**Пол Кристиано:**

- [Takeoff speeds](https://sideways-view.com/2018/02/24/takeoff-speeds/)
<br><br>

**Скотт Ааронсон:**

- [The Singularity Is Far](https://www.scottaaronson.com/blog/?p=346)
<br><br>

**Николас Агар:**

- [Don’t Worry about Superintelligence](https://jetpress.org/v26.1/agar.htm)
<br><br>

**Мацей Цегловский:**

- [Superintelligence: The Idea That Eats Smart People](http://idlewords.com/talks/superintelligence.htm) <br> [Сверхинтеллект: идея, не дающая покоя умным людям](https://habr.com/ru/articles/432806/), пер. Вячеслава Голованова
<br><br>

**Тимоти Б. Ли:**

- [Will artificial intelligence destroy humanity? Here are 5 reasons not to worry.](https://www.vox.com/2014/8/22/6043635/5-reasons-we-shouldnt-worry-about-super-intelligent-computers-taking)
<br><br>

**Нил Лоуренс:**

- [Future of AI 6. Discussion of ‘Superintelligence: Paths, Dangers, Strategies’](http://inverseprobability.com/2016/05/09/machine-learning-futures-6#fnref:embodied)
<br><br>

**Кевин Келли:**

- [The Myth of a Superhuman AI](https://www.wired.com/2017/04/the-myth-of-a-superhuman-ai/) <br> [Культ карго для ИИ: миф о сверхчеловеческом искусственном интеллекте](https://habr.com/ru/articles/403999/), пер. Вячеслава Голованова
<br><br>

**Пол Аллен:**

- [The Singularity Isn’t Near](https://www.technologyreview.com/s/425733/paul-allen-the-singularity-isnt-near/)
<br><br>

**Александер Круэл:**

- [AI Risk Critiques: Index](http://kruel.co/2012/07/17/ai-risk-critiques-index/#sthash.1c9XTToM.dpbs) (ссылается на множество статей)
<br><br>

**Джон Брокман (редактор):**

- [_Possible Minds: Twenty-Five Ways of Looking at AI_](https://www.amazon.com/Possible-Minds-Twenty-Five-Ways-Looking/dp/0525557997/) <br> [Искусственный интеллект — надежды и опасения](https://ast.ru/book/iskusstvennyy-intellekt-nadezhdy-i-opaseniya-844327/), изд-во АСТ
<br><br>

**Мартин Форд:**

- [_Architects of Intelligence: The truth about AI from the people building it_](https://www.amazon.com/Architects-Intelligence-truth-people-building-ebook/dp/B07H8L8T2J) <br> [Архитекторы интеллекта: вся правда об искусственном интеллекте от его создателей](https://www.piter.com/collection/programmirovanie-osnovy-i-algoritmy/product/arhitektory-intellekta-vsya-pravda-ob-iskusstvennom-intellekte-ot-ego-sozdateley), изд-во Питер
<br><br>

**Магнус Виндинг (я):**

- [_Reflections on Intelligence_](https://www.smashwords.com/books/view/655938)
- [Why Altruists Should Perhaps Not Prioritize Artificial Intelligence: A Lengthy Critique](https://magnusvinding.com/2018/09/18/why-altruists-should-perhaps-not-prioritize-artificial-intelligence-a-lengthy-critique/)
- [Chimps, Humans, and AI: A Deceptive Analogy](https://magnusvinding.com/2020/06/04/a-deceptive-analogy/)
- [Some reasons not to expect a growth explosion](https://magnusvinding.com/2021/06/07/reasons-not-to-expect-a-growth-explosion/)
- [Two contrasting models of “intelligence” and future growth](https://forum.effectivealtruism.org/posts/7cCr6vAmN4Xi3yzR5/two-contrasting-models-of-intelligence-and-future-growth)
- [Reasons that reduce the probability of extinction from rogue AGI](https://magnusvinding.com/wp-content/uploads/2023/06/extinction-from-rogue-agi.pdf)
<br><br>

**Краткая аннотация и обзор «_Reflections on Intelligence_» Каем Соталой:**

- [Disjunctive AI scenarios: Individual or collective takeoff?](http://kajsotala.fi/2017/01/disjunctive-ai-scenarios-individual-or-collective-takeoff/)
<br><br>

**Разные авторы:**

- [Tech Luminaries Address Singularity](https://spectrum.ieee.org/computing/hardware/tech-luminaries-address-singularity)
- [Perspectives on intelligence explosion](https://wiki.lesswrong.com/wiki/Perspectives_on_intelligence_explosion)
<br><br>

**Не напрямую по теме, но все же релевантные, по моему мнению:**

- Стивен Сломан, Филип Фернбах, [_The Knowledge Illusion: Why We Never Think Alone_](https://www.amazon.com/dp/B01HNJIJY4/) <br> [Иллюзия знания. Почему мы никогда не думаем в одиночестве](https://azbooka.ru/books/illyuziya-znaniya-pochemu-my-nikogda-ne-dumaem-v-odinochestve-30mu), изд-во КоЛибри
- Джейкоб Броновски, [_The Ascent of Man_](https://www.amazon.com/dp/B005BON6OW/) <br> [Восхождение человечества](https://www.piter.com/collection/pop-science/product/voshozhdenie-chelovechestva-predislovie-richarda-dokinza), изд-во Питер
- Мэтт Ридли, [_The Evolution of Everything_](https://www.amazon.com/dp/B00U1T9OSO/) <br> [Эволюция всего](https://eksmo.ru/ebook/evolyutsiya-vsego-ITD810969/), изд-во Эксмо
- Джозеф Хенрик, [_The Secret of Our Success_](https://www.amazon.com/dp/B00WY4OXAS/) <br> [Секрет нашего успеха](https://www.corpus.ru/products/dzhozef-henrih-sekret-nashego-uspekha.htm), изд-во Corpus
- Томас Соуэлл, [_Intellectuals and Society_](https://www.amazon.com/dp/B06XC5D4Z9/)
