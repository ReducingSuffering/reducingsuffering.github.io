---
layout: article
title: "Что будут ценить люди будущего? Сжатое введение в аксиологическую футурологию"
authors:
  - Джим Бюлер
original_date: 2023.03.25
original: https://forum.effectivealtruism.org/s/wmqLbtMMraAv5Gyqn/p/FCkchmXcSCQtJ9PZA
translated_by: К. Кирдан
preview: "https://res.cloudinary.com/cea/image/upload/c_fill,ar_1.91,g_auto/SocialPreview/a03svai0kpmxvwukxcou"
---
## Почему это стоит исследовать

Человечество может разработать искусственный общий интеллект (AGI)[^1], колонизировать космос и создать в будущем астрономические количества всяких вещей ([Bostrom 2003](https://nickbostrom.com/astronomical/waste); [MacAskill 2022](https://whatweowethefuture.com/uk/); [Althaus & Gloor 2016](https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/)). Но что это за вещи? Насколько они положительно/отрицательно ценны? И каковы они в сравнении с теми вещами, которые в итоге создали бы [жадные инопланетяне](https://grabbyaliens.com/), если бы колонизировали наш уголок вселенной?[^2] Что это значит для нашей работы, направленной на формирование долгосрочного будущего?

Хотя ответы зависят от множества факторов, одним из ключевых, скорее всего, будут _ценности наших потомков_.

Когда мы размышляем о том, стоит ли изучать эту тему, заманчивой может показаться такая позиция:

> Маловероятно, что у наших потомков будут ценности, которые одновременно существенно отличаются от наших и предсказуемы. Либо у них будут ценности, схожие с нашими, либо ценности, которые мы не можем предсказать. Следовательно, попытки предсказать, каковы будут их ценности — это пустая трата времени и ресурсов.

Хотя я понимаю, почему это может казаться убедительным, я считаю, что это крайне ошибочно.

Во-первых, _предсказание ценностей наших потомков_ — то, что Джон Данахер ([2021](https://philarchive.org/rec/DANAFT-2)) называет «аксиологической футурологией», — в мирах, где эти ценности существенно отличаются от наших, не выглядит совсем уж неразрешимой задачей. В этой области исследований уже достигнуты значительные успехи, и, похоже, есть много возможностей для дальнейшего прогресса (см. следующий раздел и Приложение).

Во-вторых, сценарий, при котором ценности наших потомков не будут существенно отличаться от наших, представляется мне довольно маловероятным[^3]. Тут следует учитывать такие вещи, как [иллюзия конца истории](https://ru.wikipedia.org/wiki/%D0%98%D0%BB%D0%BB%D1%8E%D0%B7%D0%B8%D1%8F_%D0%BA%D0%BE%D0%BD%D1%86%D0%B0_%D0%B8%D1%81%D1%82%D0%BE%D1%80%D0%B8%D0%B8). Похоже, что по ходу истории ценности заметно развиваются, и нет причин предполагать, что мы настолько особенные, что должны считать себя исключением из этого правила.

Помимо того, что работа в направлении аксиологической футурологии возможна, я также считаю эту работу необычайно важной, учитывая, что она поможет ответить на упомянутые ранее ключевые вопросы. Поэтому значимость этой работы также выглядит неоправданно недооцененной в нынешнее время.

## Как это исследовать

Вот примеры общих вопросов, которые могли бы стать частью исследовательской повестки по этой теме:

- Какие факторы лучше всего предсказывают будущие человеческие ценности? Чему нас могут научить обычные методы прогнозирования?
- Как менялись ценности людей на протяжении истории? Почему? Чему это может нас научить? (см. например, [MacAskill 2022](https://whatweowethefuture.com/uk/), глава 3; [Harris 2019](https://www.sentienceinstitute.org/blog/how-tractable-is-changing-the-course-of-history); [Hopster 2022](https://philpapers.org/rec/HOPFVC))
- Есть ли причины полагать, что в будущем мы будем наблюдать меньше изменений? Почему? Из-за [фиксации ценностей?](https://forum.effectivealtruism.org/topics/value-lock-in) Какой-то формы моральной конвергенции в ближайшем будущем?
- Есть ли причины ожидать большего числа изменений? Может ли это быть связано с развитием AGI, полной эмуляцией мозга, колонизацией космоса и/или ускоренным дрейфом ценностей? Что, если случится глобальная катастрофа?
- Более широкий вопрос: какое влияние на ценности окажет будущий технический прогресс? (См. пример прогноза у [Hanson 2016](https://ageofem.com/).)
- Стоит ли ожидать, что в пользу некоторых ценностей будет идти отбор? (см. например, [Christiano 2013](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/); [Bostrom 2009](https://nickbostrom.com/fut/evolution), [Tomasik 2017](https://reducing-suffering.org/the-future-of-darwinism/))
- Может ли произойти период «[долгого размышления](https://forum.effectivealtruism.org/topics/long-reflection)»? Если да, можем ли мы понять, к чему это может привести?
- Есть ли шанс реализации чего-то вроде [когерентной экстраполированной воли](https://www.lesswrong.com/tag/coherent-extrapolated-volition), и если да, то к чему это реалистично может привести?
- Есть ли сценарии будущего с определенными ценностями у человечества, которые маловероятны, но на которые стоит делать ставку?
- Может ли само наше исследование этой темы повлиять на ценности наших потомков, которые нам следует ожидать, — например, через эффект саморазрушающегося или самосбывающегося пророчества? ([Danaher 2021](https://philarchive.org/rec/DANAFT-2), раздел 2)
- Что ценят или будут ценить инопланетяне и что это говорит нам о нас самих?
- А что насчёт ценностей потенциальной цивилизации, которая может появиться на Земле после вымирания человечества?<br><br>

Джон Данахер ([2021](https://philarchive.org/rec/DANAFT-2)) приводит примеры методологий, которые могут быть использованы для ответа на эти вопросы.

Также в Приложении приведены примеры и другие релевантные работы, включая следующие посты этой серии.

## Благодарности

Спасибо Андерсу Сандбергу за наводку на работу Джона Данахера ([2021](https://philarchive.org/rec/DANAFT-2)) и глубокое обсуждение этой темы. Спасибо Элиасу Шмиду за прочие рекомендации. Спасибо также М. Виктории Калабресе за ее предложения по стилистике. Моя работа над этой цепочкой до сих пор финансировалась [Existential Risk Alliance](https://erafellowship.org/).

Все допущения/утверждения/упущения являются моими.

## Приложение: Релевантные работы

(Этот список не является исчерпывающим[^4]. Сортировка более-менее в порядке убывания важности.)

- Делают прогнозы или поднимают соответствующие обсуждения — о том, что могут ценить наши потомки:
  - John Danaher (2021) [Axiological Futurism: The Systematic Study of the Future of Values](https://philarchive.org/rec/DANAFT-2)
  - Robin Hanson (1998) [Burning the Cosmic Commons: Evolutionary Strategies for Interstellar Colonization](http://mason.gmu.edu/~rhanson/filluniv.pdf)
  - Nick Bostrom (2004) [The Future of Human Evolution](https://nickbostrom.com/fut/evolution)
  - Brian Tomasik (2017) [The Future of Darwinism](https://reducing-suffering.org/the-future-of-darwinism/)
  - Allan Dafoe (2019) [Value Erosion](https://docs.google.com/document/d/1B77VWaXG-u34nSRFKV14pJNHJHHb6sa5zJ08J70CVVA/edit)
  - Robin Hanson (2022) [Will Design Escape Selection?](https://www.overcomingbias.com/p/will-design-escape-selectionhtml)
  - Robin Hanson (2021) [On Evolved Values](https://www.overcomingbias.com/p/on-evolved-valueshtml)
  - Robin Hanson (2009) [This is the Dream Time](https://www.overcomingbias.com/p/this-is-the-dream-timehtml)
  - interstice (2022) [Alignment Might Never Be Solved, By Humans or AI](https://www.lesswrong.com/posts/BuaFZud9BwkiSCGpd/alignment-might-never-be-solved-by-humans-or-ai)
  - Wei Dai (2014) [Six Plausible Meta-Ethical Alternatives](https://www.lesswrong.com/posts/orhEa4wuRJHPmHFsR/six-plausible-meta-ethical-alternatives)
  - «[We can expect future agents to have other-regarding preferences that we would, after reflection, find somewhat positive](https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive#We_can_expect_future_agents_to_have_other_regarding_preferences_that_we_would__after_reflection__find_somewhat_positive)» и «[Appendix 2](https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive?fbclid=IwAR2Si8qdOEqXdPujDfv6gDGLaTdevs4Tb_CALW0D2MHUC4Ot9evEAoem3Gw#Appendix_2__Future_agents_will_in_expectation_have_a_considerable_fraction_of_other_regarding_preferences)» в Brauner & Grosse-Holz ([2018](https://forum.effectivealtruism.org/posts/NfkEqssr7qDazTquW/the-expected-value-of-extinction-risk-reduction-is-positive?fbclid=IwAR2Si8qdOEqXdPujDfv6gDGLaTdevs4Tb_CALW0D2MHUC4Ot9evEAoem3Gw))
  - Robin Hanson (2016) [The Age of Em](https://ageofem.com/)
  - Paul Christiano (2013) [Why might the future be good?](https://rationalaltruist.com/2013/02/27/why-will-they-be-happy/)
  - Jacy Reese Anthis (2018) [The End of Animal Farming](https://en.wikipedia.org/wiki/The_End_of_Animal_Farming)
  - Robin Hanson (2018) [Longviews are coming](https://www.overcomingbias.com/p/long-views-are-cominghtml)
  - Yuval Noah Harari (2016) [Homo Deus](https://books.google.co.uk/books?hl=en&lr=&id=ZWcNDQAAQBAJ&oi=fnd&pg=PA1&ots=-ETYkeIBQ5&sig=FlFk1A9hxymM0BPuY1Y_cG4mmWw&redir_esc=y)
  - «The “Consciousness vs. Pure Replicators” Worldview» в Emilsson ([2017](https://qualiacomputing.com/2017/12/20/the-universal-plot-part-i-consciousness-vs-pure-replicators/))
  - Следующие посты этой цепочки.
  - Anders Sandberg (планируется) Grand Futures
- О возможности, вероятности и последствиях фиксации ценностей / дрейфа ценностей:
  - Lukas Finnveden et al. (2022) [AGI and Lock-In](https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in); и работы, на которые они ссылаются в разделе «[How likely is this?](https://docs.google.com/document/d/1mkLFhxixWdT5peJHq4rfFzq4QbHyfZtANH1nou68q88/edit#heading=h.17f05s8r0u3q)»
- О стимулах к колонизации космоса:
  - Carl Shulman (2018) [Financial returns of interstellar colonization for the sedentary](http://reflectivedisequilibrium.blogspot.com/2018/10/financial-returns-of-interstellar.html)
  - Anders Sandberg (2018) [Space races: settling the universe fast](https://www.fhi.ox.ac.uk/wp-content/uploads/space-races-settling.pdf)

## Сноски

[^1]: Или что-нибудь примерно столь же трансформирующее.
[^2]: См. Rational Animations, «[Мы появились рано и из-за жадных инопланетян](https://www.youtube.com/watch?v=cAXFLbom4oc)» и «[Решение парадокса Ферми от создателя Великого фильтра. "Громкие" и "жадные" инопланетяне](https://habr.com/ru/articles/583022/)» — прим. пер.
[^3]: Внезапная фиксация ценностей в результате разработки и внедрения AGI в ближайшие годы/десятилетия — вероятно, наиболее правдоподобная перспектива. (См. [Finnveden et al. 2022](https://forum.effectivealtruism.org/posts/KqCybin8rtfP3qztq/agi-and-lock-in).)
[^4]: Больше из-за ограниченности моих знаний, чем из-за желания сделать этот список покороче, так что, пожалуйста, отправляйте мне другие ресурсы, которые могут оказаться подходящими! (Для связи с автором см. [www.jimbuhler.site](https://www.jimbuhler.site/) — прим. пер.)
